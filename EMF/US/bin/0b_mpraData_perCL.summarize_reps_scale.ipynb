{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6d77061",
   "metadata": {},
   "source": [
    "Log\n",
    "- 20230816 \n",
    "- 20231115 - revised  after Chengyu cleaned up data. \n",
    "- 20240520 - revised to standardize data transformations\n",
    "- sarahfong\n",
    "\n",
    "# objective \n",
    "learn the features of the US MPRA dataset\n",
    "\n",
    "do some basic transformations and comparisons \n",
    "## clean up and label\n",
    "- label cl.origin, control sequences\n",
    "- label 'active': in ctrl, us if MEDIAN exceeds top 97.5 or bottom 2.5 shuffled PI\n",
    "- label 'response': if ctrl != us active label and uncorrected pval<0.05\n",
    "- label 'direction':\n",
    "    - US_UP = US median is positive, CTRL median is negative\n",
    "    - US_Down = US median is neg, CTRL median is pos\n",
    "    - US_MORE_SILENT = US med is more neg than CTRL med\n",
    "    - US_LESS_SILENT = US med is less neg than CTRL med\n",
    "    - US_MORE_ACTIVE = US med is more pos than CTRL pos\n",
    "    - US_LESS_ACTIVE = US med is less pos than CTRL pos\n",
    "\n",
    "## transformations\n",
    "- per replicate \n",
    "    \n",
    "    - log2 normalize RNA/DNA\n",
    "    - standard scale log2 RNA/DNA\n",
    "\n",
    "- across replicates\n",
    "    - compute median, mean, sd of log2 values, standard scaling values\n",
    "\n",
    "    - delta\n",
    "        - compute as the median/mean difference l2(US/control) or l2US - l2control\n",
    "\n",
    "- Significance among tiles, across peaks\n",
    "    -  per tile: Wilcoxon's t-test (also ran BH 5% FDR, but not any significant)\n",
    "    -  per peak: Repeated measure t-test (also ran BH 5% FDR, but not any significant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10e7dc6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T00:48:39.029247Z",
     "start_time": "2023-11-16T00:48:37.931537Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import fdrcorrection\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51677c2a",
   "metadata": {},
   "source": [
    "# Read, write to config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ef440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T00:48:39.035287Z",
     "start_time": "2023-11-16T00:48:39.030987Z"
    }
   },
   "outputs": [],
   "source": [
    "LOCAL = False\n",
    "if LOCAL is True:\n",
    "    sys.path.append(\"/Users/sarahfong/tools/py_\")\n",
    "    DATA_PATH = \"/Users/sarahfong/Desktop/local_data/EMF/US\"\n",
    "else:\n",
    "    DATA_PATH = \"/wynton/group/ahituv/fongsl/projects/US/data\"\n",
    "import config_readwrite as crw\n",
    "import plot_params as pp\n",
    "\n",
    "pp.fonts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a96c10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T00:48:39.040515Z",
     "start_time": "2023-11-16T00:48:39.036300Z"
    }
   },
   "outputs": [],
   "source": [
    "# read\n",
    "config, cfn = crw.read(os.path.join(os.path.dirname(os.getcwd()), \"config.ini\"))\n",
    "\n",
    "# path\n",
    "#DATA_PATH = config[\"local_path\"][\"data\"]\n",
    "\n",
    "# make dictionary of values to write to config\n",
    "config_dict = {\n",
    "    \"HEPG2\": os.path.join(DATA_PATH, \"full_hepg2_MPRA.csv\"),\n",
    "    \"HEPG2.clean.transformed\": os.path.join(DATA_PATH, \"hepg2_MPRA.clean.transformed.tsv\"),\n",
    "    \"HEPG2.clean.trans.peaks\": os.path.join(DATA_PATH, \"hepg2.MPRA.clean.transformed.peaks.tsv\"),\n",
    "    \"HEPG2.clean.trans.scaled\": os.path.join(DATA_PATH, \"hepg2_MPRA.clean.transformed.standard.scaled.tsv\"),\n",
    "    \"HEPG2.clean.trans.scaled.peaks\":os.path.join(DATA_PATH, \"hepg2.MPRA.clean.transformed.standard.scaled.peaks.tsv\"),\n",
    "    \n",
    "    \"BJ\": os.path.join(DATA_PATH, \"full_bj_MPRA.csv\"),\n",
    "    \"BJ.clean.transformed\": os.path.join(DATA_PATH, \"bj_MPRA.clean.transformed.csv\"),\n",
    "    \"BJ.clean.trans.peaks\":os.path.join(DATA_PATH, \"bj.MPRA.clean.transformed.peaks.tsv\"),\n",
    "    \"BJ.clean.trans.scaled\": os.path.join(DATA_PATH, \"bj_MPRA.clean.transformed.standard.scaled.csv\"),\n",
    "    \"BJ.clean.trans.scaled.peaks\":os.path.join(DATA_PATH, \"bj.MPRA.clean.transformed.standard.scaled.peaks.tsv\"),\n",
    "\n",
    "}\n",
    "\n",
    "# make data section of config\n",
    "section = \"mpra\"\n",
    "crw.check(config, section)\n",
    "\n",
    "# add dictionary to config\n",
    "for key, value in config_dict.items():\n",
    "    config[section][key] = value\n",
    "    \n",
    "# write to config    \n",
    "crw.write(config, cfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5149fa5d",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b017db2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T00:48:39.053853Z",
     "start_time": "2023-11-16T00:48:39.041447Z"
    }
   },
   "outputs": [],
   "source": [
    "# mean, median, std\n",
    "\n",
    "def computeStats(df, l2_ratios_list):\n",
    "    \"\"\" compute median, mean, std of ctrl and ultrasound replicates per sequence\"\"\"\n",
    "    print(l2_ratios_list[:3])\n",
    "    df[\"l2.ratio.med.ctrl\"] = df[l2_ratios_list[:3]].median(axis=1)\n",
    "    df[\"l2.ratio.mean.ctrl\"] = df[l2_ratios_list[:3]].mean(axis=1)\n",
    "    df[\"l2.ratio.std.ctrl\"] = df[l2_ratios_list[:3]].std(axis=1)\n",
    "\n",
    "    df[\"l2.ratio.med.us\"] = df[l2_ratios_list[3:]].median(axis=1)\n",
    "    df[\"l2.ratio.mean.us\"] = df[l2_ratios_list[3:]].mean(axis=1)\n",
    "    df[\"l2.ratio.std.us\"] = df[l2_ratios_list[3:]].std(axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# l2 transform (RNA/DNA)\n",
    "\n",
    "def log2Transform(df, ratios_list):\n",
    "    \"\"\" log2 transform each ratio column\"\"\"\n",
    "\n",
    "    for ratio in ratios_list:\n",
    "        df[f\"l2.{ratio}\"] = np.log2(df[ratio])\n",
    "\n",
    "    return df\n",
    "\n",
    " # compute fold change of medians\n",
    "\n",
    "\n",
    "def computeDelta(df):\n",
    "    \"\"\" compute delta of log2 median us - log2 median control\"\"\"\n",
    "\n",
    "    df[\"delta.med\"] = df[\"l2.ratio.med.us\"]-df[\"l2.ratio.med.ctrl\"]\n",
    "    df[\"delta.mean\"] = df[\"l2.ratio.mean.us\"]-df[\"l2.ratio.mean.ctrl\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clOrigin(df):\n",
    "    \"\"\"annotate which cl a sequence was designed from\"\"\"\n",
    "\n",
    "    \n",
    "    df[\"cl.origin\"] = df[\"name\"].apply(lambda x: x.split(\"_\")[0].lower())\n",
    "    df.loc[df[\"cl.origin\"] == \"synthetic:\", \"cl.origin\"] = \"synthetic\"\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def ctrlAnnot(df, constants_list):\n",
    "    \"\"\" annotate control type (pos, neg, test)\"\"\"\n",
    "\n",
    "    df[\"type\"] = \"None\"\n",
    "    for ctrl in constants_list:\n",
    "        df.loc[df['label'].str.contains(ctrl), \"type\"] = ctrl\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def computePval(df, constants_list):\n",
    "    \"\"\" compute per sequence ttest of ctrl v. ultrasound rep\n",
    "        assume equal_var is False \n",
    "    \"\"\"\n",
    "    df[\"pval\"] = None\n",
    "    for i, row in df.iterrows():\n",
    "\n",
    "        # get control replicates\n",
    "        ctrls = row[constants_list[:3]]\n",
    "\n",
    "        # get US replicates\n",
    "        uss = row[constants_list[3:]]\n",
    "        # t-test per sequence, no equal variance assumed.\n",
    "        t, p = stats.ttest_ind(list(ctrls), list(uss), equal_var=False)\n",
    "\n",
    "        # update dataframe\n",
    "        df.at[i, 'pval'] = p\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def callActive(df):\n",
    "    \"\"\"call active elements from ctrl, us MPRA as values > 95 of shuffled regions\"\"\"\n",
    "\n",
    "    # get shuffles\n",
    "    shufs = df.loc[df.name.str.contains(\"shuf\")].copy()\n",
    "\n",
    "    # get 97.5% of shuffle median score for ctrl, US treatment\n",
    "    ctrl_975 = shufs['l2.ratio.med.ctrl'].quantile(0.975)\n",
    "    us_975 = shufs['l2.ratio.med.us'].quantile(0.975)\n",
    "\n",
    "    ctrl_025 = shufs['l2.ratio.med.ctrl'].quantile(0.025)\n",
    "    us_025 = shufs['l2.ratio.med.us'].quantile(0.025)\n",
    "    print(ctrl_975, ctrl_025, us_975, us_025)\n",
    "\n",
    "    # create column to label active\n",
    "    df['label.ctrl'], df['label.us'] = 0, 0\n",
    "\n",
    "    # label active elements - has more activity than 97.5% shuffles\n",
    "    df.loc[df['l2.ratio.med.ctrl'] > ctrl_975, 'label.ctrl'] = 1\n",
    "    df.loc[df['l2.ratio.med.us'] > us_975, 'label.us'] = 1\n",
    "\n",
    "    # label active elements - has less activity than 2.5% shuffles\n",
    "    df.loc[df['l2.ratio.med.ctrl'] < ctrl_025, 'label.ctrl'] = -1\n",
    "    df.loc[df['l2.ratio.med.us'] < us_025, 'label.us'] = -1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def callResponse(df):\n",
    "    \"\"\"label (1) response as  wilcoxon pval <0.05 and categorical ctrl v. US labels are not equal. \n",
    "        - No l2fc criteria (CD used this)\n",
    "        lable (2) direction of activity\n",
    "        \n",
    "    \"\"\"\n",
    "    df[\"response\"] = False\n",
    "\n",
    "    df.loc[(df[\"pval\"] < 0.05) &\n",
    "           (df['label.ctrl'] != df['label.us']),\n",
    "           \"response\"\n",
    "           ] = True\n",
    "\n",
    "    # describe direction of values\n",
    "    df[\"direction\"] = None\n",
    "\n",
    "    # direction when US is positive and ctrl is negative\n",
    "    df.loc[(df[\"l2.ratio.med.ctrl\"] < 0)\n",
    "           & (df[\"l2.ratio.med.us\"] >= 0), \"direction\"] = \"US_UP\"\n",
    "\n",
    "    # when US is negative and ctrl is positive\n",
    "    df.loc[(df[\"l2.ratio.med.ctrl\"] > 0)\n",
    "           & (df[\"l2.ratio.med.us\"] <= 0), \"direction\"] = \"US_DOWN\"\n",
    "\n",
    "    # Less silent when US is negative and ctrl is negative and US > CTRL\n",
    "    df.loc[(df[\"l2.ratio.med.ctrl\"] < 0)\n",
    "           & (df[\"l2.ratio.med.us\"] <= 0)\n",
    "           & (df[\"l2.ratio.med.us\"] > df[\"l2.ratio.med.ctrl\"]), \"direction\"] = \"US_LESS_SILENT\"\n",
    "\n",
    "    # More silent when US is negative and ctrl is negative and US < CTRL\n",
    "    df.loc[(df[\"l2.ratio.med.ctrl\"] < 0)\n",
    "           & (df[\"l2.ratio.med.us\"] <= 0)\n",
    "           & (df[\"l2.ratio.med.us\"] < df[\"l2.ratio.med.ctrl\"]), \"direction\"] = \"US_MORE_SILENT\"\n",
    "\n",
    "    # Less active when US is positive and ctrl is positive and CTRL > US\n",
    "    df.loc[(df[\"l2.ratio.med.ctrl\"] > 0)\n",
    "           & (df[\"l2.ratio.med.us\"] >= 0)\n",
    "           & (df[\"l2.ratio.med.us\"] < df[\"l2.ratio.med.ctrl\"]), \"direction\"] = \"US_LESS_ACTIVE\"\n",
    "    # more active when US is positive and ctrl is positive and CTRL < US\n",
    "    df.loc[(df[\"l2.ratio.med.ctrl\"] > 0)\n",
    "           & (df[\"l2.ratio.med.us\"] >= 0)\n",
    "           & (df[\"l2.ratio.med.us\"] > df[\"l2.ratio.med.ctrl\"]), \"direction\"] = \"US_MORE_ACTIVE\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2da51-ce5c-47ec-adba-3225cad2c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdrCorr(df, id_col, p_val_col):\n",
    "    \"\"\"FDR correction, alpha = 5% per cl, datatype hypothesis\n",
    "\n",
    "        Subset hypothesis to test separately with FDR correction.\n",
    "        \n",
    "        cl.origin: HepG2, K562, BJ, HOB \n",
    "        Data type: H3K27ac, ATAC, Synthetics\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    fdrs = {}\n",
    "    for origin in [\"hepg2\", \"k562\", \"hob\", \"bj\", None]:\n",
    "        for datatype in [\"Synthetic\", 'H3K27ac', \"ATAC\"]:\n",
    "\n",
    "            # subset dataframe by hypothesis\n",
    "            test = df.loc[(df[\"cl.origin\"] == origin) &\n",
    "                          (df['type'] == datatype), [id_col, p_val_col]].copy().drop_duplicates()\n",
    "\n",
    "            # perform fdr correction only if more than 30 observations\n",
    "            if test.shape[0] > 30:\n",
    "                print(\"fdr correction\", origin, datatype)\n",
    "                \n",
    "                test[\"FDR_bool\"], test[f'FDR_{p_val_col}'] = fdrcorrection(test[p_val_col])\n",
    "                \n",
    "                # compute -log10 p\n",
    "                test[\"FDR_-log10p\"] = np.log10(test[f'FDR_{p_val_col}'])*-1\n",
    "                fdrs[f\"{origin}.{datatype}\"] = test #append results to dictionary\n",
    "\n",
    "    \n",
    "    return pd.merge(df, pd.concat(fdrs.values()), how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151a6f02-7626-4dcb-80ea-cd65c66fde51",
   "metadata": {},
   "source": [
    "## peak-wise function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cfef94-72a3-49c5-bdea-85653f0f655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def peakwiseStats(enh_id, df):\n",
    "    \"\"\" collect and perform related t-test on peak tiles in US/CTRL exposures\n",
    "        - treat all tiles in peaks as repeated measures in two different conditions - US and CTRL\n",
    "    \"\"\"\n",
    "    cols = [\"name\",\n",
    "            'l2.ratio.med.ctrl',\n",
    "            'label.ctrl',\n",
    "            'l2.ratio.1.ctrl',\n",
    "            'l2.ratio.2.ctrl',\n",
    "            'l2.ratio.3.ctrl',\n",
    "            'l2.ratio.med.us',\n",
    "            'label.us',\n",
    "            'l2.ratio.1.us',\n",
    "            'l2.ratio.2.us',\n",
    "            'l2.ratio.3.us',\n",
    "            'delta.med',\n",
    "            \"direction\", \"enh.coor\", \"enh.name\"]\n",
    "\n",
    "    # dget dataframe of enhancer only\n",
    "    enh = df.loc[df[\"enh.name\"] == enh_id, cols].copy()\n",
    "\n",
    "    # treat all measurements as\n",
    "    ctrls = list(pd.melt(enh,\n",
    "                         value_vars=['l2.ratio.1.ctrl',\n",
    "                                     'l2.ratio.2.ctrl',\n",
    "                                     'l2.ratio.3.ctrl'])[\"value\"])\n",
    "    us = list(pd.melt(enh,\n",
    "                      value_vars=['l2.ratio.1.us',\n",
    "                                  'l2.ratio.2.us',\n",
    "                                  'l2.ratio.3.us', ])[\"value\"])\n",
    "    ###\n",
    "    # RELATIVE T-TEST - treat tiles like repeated measures\n",
    "    ###\n",
    "    \n",
    "    # two related samples t-test, \n",
    "    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html\n",
    "    s, p = stats.ttest_rel(ctrls, us)\n",
    "    enh[\"peak_p\"], enh[\"peak_stat\"] = p, s\n",
    "\n",
    "    return s, p, enh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad93d32f",
   "metadata": {},
   "source": [
    "# test CL \n",
    "\n",
    "## notes about this dataset\n",
    "- Activity score - median score was assigned per US|CTRL for each sequence\n",
    "- Active | inactive = test sequence score > 95% shuffled score. Both these scores are median score (above)\n",
    "- logFC\n",
    "- P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b1d51",
   "metadata": {},
   "source": [
    "### constants dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09040e15",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T00:48:39.059620Z",
     "start_time": "2023-11-16T00:48:39.057273Z"
    }
   },
   "outputs": [],
   "source": [
    "constants = {\n",
    "    \"NAMES\": ['name',\n",
    "              'label',\n",
    "              'ratio.med.ctrl',\n",
    "              'label.ctrl',\n",
    "              'ratio.1.ctrl',\n",
    "              'ratio.2.ctrl',\n",
    "              'ratio.3.ctrl',\n",
    "              'ratio.med.us',\n",
    "              'label.us',\n",
    "              'ratio.1.us',\n",
    "              'ratio.2.us',\n",
    "              'ratio.3.us',\n",
    "              'pval',\n",
    "              \"p_adj\",\n",
    "              'logFC',\n",
    "              'response'],\n",
    "\n",
    "    \"CTRL_LIST\": [\"Neg\", \"PosCt\", \"Shuffle\", \"DEG\", \"Non-diff\", \"Synthetic\", 'H3K27ac', \"ATAC\"],\n",
    "\n",
    "    \"RATIOS\": ['ratio.1.ctrl', 'ratio.2.ctrl', 'ratio.3.ctrl',\n",
    "               'ratio.1.us', 'ratio.2.us', 'ratio.3.us'],\n",
    "\n",
    "    \"L2RATIOS\": ['l2.ratio.1.ctrl', 'l2.ratio.2.ctrl', 'l2.ratio.3.ctrl',\n",
    "                 'l2.ratio.1.us', 'l2.ratio.2.us', 'l2.ratio.3.us'],\n",
    "\n",
    "    \"L2STD\": ['l2.ratio.std.ctrl', 'l2.ratio.std.us'],\n",
    "\n",
    "    \"CL_LIST\": [\"k562\", \"hob\", \"bj\", \"hepg2\"],\n",
    "\n",
    "    \"ONOFF\": [\"US_UP\", 'US_MORE_ACTIVE', \"US_DOWN\", \"US_MORE_SILENT\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f911528",
   "metadata": {},
   "source": [
    "## clean, transform data, compute summary stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00691c5-f213-47cf-a350-4dc1a5657c3d",
   "metadata": {},
   "source": [
    "## peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da01851-e432-4a4c-87b3-84c37c254a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## open peak annotation\n",
    "PEAK_ANNOT = config[\"data\"][\"TILE_x_ENH\"] #\"/Users/sarahfong/Desktop/local_data/EMF/US/tilenames.x.enh.id.tsv\"\n",
    "peaks = pd.read_csv(PEAK_ANNOT, sep='\\t')\n",
    "peaks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b947336-a1c3-48dd-afd7-b131e2f303dd",
   "metadata": {},
   "source": [
    "## process MPRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772f7c74-1ec5-4ffc-86f0-74883a02ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### input data\n",
    "\n",
    "CLS=[\"HEPG2\", \"BJ\"] \n",
    "\n",
    "results = {} # collect all the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284efbe8-9559-417a-8d9f-f2597d22f30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for CL in CLS:\n",
    "    results[CL] = {}  # make dictinoary of dictionaries for results\n",
    "    cl_dict = results[CL]\n",
    "\n",
    "    # read\n",
    "    DATA = config_dict[CL]\n",
    "\n",
    "    # write\n",
    "    CLEAN = config_dict[f\"{CL}.clean.transformed\"]\n",
    "    SCALED = config_dict[f'{CL}.clean.trans.scaled']\n",
    "    CLEAN_PEAKS = config_dict[f\"{CL}.clean.trans.peaks\"]\n",
    "    CLEAN_SCALED_PEAKS = config_dict[f\"{CL}.clean.trans.scaled.peaks\"]\n",
    "\n",
    "    # load CD's data\n",
    "    # .csv data, rename columns, skip row 1 (old column names)\n",
    "    df_ = pd.read_csv(DATA, skiprows=1,\n",
    "                      names=constants[\"NAMES\"], low_memory=False)\n",
    "    cl_dict[\"cd_df\"] = df_  # add cd's dataframe to the results dictionary\n",
    "\n",
    "    ###\n",
    "    # clean up MPRA data - add annotations, l2 transform, get  replicate means, medians\n",
    "    ###\n",
    "\n",
    "    # annotate cell line origin\n",
    "    df = clOrigin(df_)\n",
    "\n",
    "    # annotate controls\n",
    "    df = ctrlAnnot(df, constants[\"CTRL_LIST\"])\n",
    "    \n",
    "\n",
    "    # log2 transform ratios - increase sensitivity for ratios <1\n",
    "    df = log2Transform(df, constants[\"RATIOS\"])\n",
    "\n",
    "    # compute descriptive stats\n",
    "    df = computeStats(df, constants[\"L2RATIOS\"])\n",
    "\n",
    "    # compute difference between scaled us v. control\n",
    "    df = computeDelta(df)\n",
    "\n",
    "    # recall active\n",
    "    df = callActive(df)\n",
    "\n",
    "    # re compute wilcoxon pvalues\n",
    "    df = computePval(df, constants[\"L2RATIOS\"])\n",
    "\n",
    "    # label response based on label.ctrl != label.us, wilcoxon p-val, and l2fc >1\n",
    "    df = callResponse(df)\n",
    "\n",
    "    # format pvalues\n",
    "    df[\"pval\"] = df[\"pval\"].astype(float)\n",
    "    \n",
    "    # fdr correction - per hypothesis\n",
    "    \n",
    "    df = fdrCorr(df, id_col=\"name\", p_val_col=\"pval\")\n",
    "\n",
    "\n",
    "    # write the file\n",
    "    df.to_csv(CLEAN, sep='\\t', index=False)\n",
    "    print('wrote clean')\n",
    "\n",
    "    cl_dict[\"clean_df\"] = df  # add cd's dataframe to the results dictionary\n",
    "\n",
    "    ###\n",
    "    # Standard scle replicates\n",
    "    ###\n",
    "\n",
    "    ratios = constants[\"L2RATIOS\"]  # load ratios\n",
    "    X = df[ratios]\n",
    "\n",
    "    transformer = StandardScaler()\n",
    "\n",
    "    t = pd.DataFrame(transformer.fit_transform(X[ratios]))\n",
    "\n",
    "    t.columns = ratios  # rename columns\n",
    "\n",
    "    # mean, median of standardized values\n",
    "\n",
    "    t = computeStats(t, ratios)\n",
    "\n",
    "    # compute delta\n",
    "    t = computeDelta(t)\n",
    "\n",
    "    # compute p value on standardized scale\n",
    "    t = computePval(t, ratios)\n",
    "\n",
    "    # add back names\n",
    "    t = pd.merge(df['name'], t, left_index=True, right_index=True)\n",
    "\n",
    "    # recall active\n",
    "    t = callActive(t)\n",
    "\n",
    "    # recall response\n",
    "    t = callResponse(t)\n",
    "\n",
    "    # add type information back into transformed dataframe\n",
    "    t = pd.merge(t, df[[\"name\",\n",
    "                        \"type\", \"cl.origin\"]])\n",
    "\n",
    "    # format p values\n",
    "    t[\"pval\"] = t[\"pval\"].astype(float)\n",
    "\n",
    "    # fdr correction - per hypothesis\n",
    "    t = fdrCorr(t, id_col=\"name\", p_val_col=\"pval\")\n",
    "  \n",
    "    # write\n",
    "    t.to_csv(SCALED, sep='\\t', index=False)\n",
    "\n",
    "    cl_dict[\"scaled_df\"] = t  # add cd's dataframe to the results dictionary\n",
    "\n",
    "    ###\n",
    "    # Peakwise analysis\n",
    "    ###\n",
    "\n",
    "    # add enhancer information to dataframes\n",
    "    t = pd.merge(t, peaks[[\"name\", \"enh.coor\", \"enh.name\"]], how=\"left\")\n",
    "    df = pd.merge(df, peaks[[\"name\", \"enh.coor\", \"enh.name\"]], how=\"left\")\n",
    "\n",
    "    all_sig_peaks = {}  # requires sig peak, DOES NOT REQUIRE categorical differential activity\n",
    "\n",
    "    for merged_df, peakfile in [(df, CLEAN_PEAKS), (t, CLEAN_SCALED_PEAKS)]:\n",
    "\n",
    "        # per enhancer compute peaks\n",
    "        for enh_id in peaks[\"enh.name\"].unique():\n",
    "\n",
    "            # perform repeated values t-test\n",
    "            s, p, enh = peakwiseStats(enh_id, merged_df)\n",
    "\n",
    "            # annotate candidate peaks - (1) significant (2) turns ON/OFF\n",
    "            enh[\"candidate\"] = False\n",
    "\n",
    "            if p < 0.05:  # evaluate significant peaks only.\n",
    "\n",
    "                # annotate elements where US turns enhancer activity ON, OFF\n",
    "                enh.loc[enh[\"direction\"].isin(\n",
    "                    constants[\"ONOFF\"]), \"candidate\"] = True\n",
    "\n",
    "            all_sig_peaks[enh_id] = enh  # add peak enhancer dictionary\n",
    "\n",
    "        sig_all = pd.concat(all_sig_peaks.values()).drop_duplicates()\n",
    "\n",
    "        # fdr correction\n",
    "        # keep only enh.name and p values, so no fdr inflation from multiple tiles in enhancer\n",
    "        fdr_df = sig_all[[\"enh.name\", \"cl.origin\", \"type\",  \"peak_p\"]].drop_duplicates().copy()\n",
    "    \n",
    "        # fdr correction - per hypothesis\n",
    "        fdr_df = fdrCorr(fdr_df, id_col=\"enh.name\", p_val_col=\"peak_p\")\n",
    "        \n",
    "        # merge back fdr information\n",
    "        sig_all = pd.merge(sig_all, fdr_df, how=\"left\")\n",
    "\n",
    "        # format log10\n",
    "        sig_all[\"-log10p_peak\"] = -1*np.log10(sig_all[\"FDR_p\"])\n",
    "\n",
    "        sig_all.to_csv(peakfile, sep='\\t', index=False)  # all tiles\n",
    "\n",
    "        print(sig_all.groupby([\"candidate\", \"FDR_bool\"])[\"direction\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb4fdb9-fcd4-4971-b0ad-24e052b647ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a553f-0e82-4165-bf9c-594ee9f5128c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_all = pd.concat(all_sig_peaks.values()).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f138ea-0020-45b5-9f1c-4ae9360e35ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only enh.name and p values, so no fdr inflation from multiple tiles in enhancer\n",
    "sig_all = pd.merge(sig_all, df[[\"name\", \"cl.origin\", \"type\"]], how=\"left\").drop_duplicates()\n",
    "fdr_df = sig_all[[\"enh.name\", \"cl.origin\", \"type\",  \"peak_p\"]].drop_duplicates().copy()\n",
    "\n",
    "# fdr correction - per hypothesis\n",
    "fdr_df = fdrCorr(fdr_df, id_col=\"enh.name\", p_val_col=\"peak_p\")\n",
    "\n",
    "# merge back fdr information\n",
    "sig_all = pd.merge(sig_all, fdr_df, how=\"left\").drop_duplicates()\n",
    "\n",
    "\n",
    "sig_all.to_csv(peakfile, sep='\\t', index=False)  # all tiles\n",
    "\n",
    "print(sig_all.groupby([\"candidate\", \"FDR_bool\"])[\"direction\"].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f272a06b-2188-4463-ab56-c485dce24147",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_df.groupby(\"enh.name\")[\"cl.origin\"].count().reset_index().sort_values(by=\"cl.origin\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68035454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T00:49:15.585455Z",
     "start_time": "2023-11-16T00:49:15.496032Z"
    }
   },
   "outputs": [],
   "source": [
    "checkCoor(df_, df, \"pval\")  # calling activity in control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb92c65-ce14-4b9c-9973-a420fc728b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"enh.name\"]==\"enh.298\"].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9410c4-7e87-4e70-b337-ca3c39056f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba)",
   "language": "python",
   "name": "mamba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.85,
   "position": {
    "height": "144.85px",
    "left": "1142px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
