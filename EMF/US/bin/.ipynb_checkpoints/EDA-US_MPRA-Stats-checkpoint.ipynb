{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b4ae20d",
   "metadata": {},
   "source": [
    "# objective \n",
    "20230816\n",
    "sarahfong\n",
    "\n",
    "learn the features of the US MPRA dataset\n",
    "\n",
    "do some basic transformations and comparisons \n",
    "\n",
    "## transformations\n",
    "- per replicate \n",
    "    \n",
    "    - log2 normalize activity values\n",
    "    - explore standard scaling log2 values (rep ctrl 2 has wider variance than other replicates) \n",
    "\n",
    "- across replicates\n",
    "    - compute median, mean, sd of log2 values, standard scaling values \n",
    "\n",
    "## questions\n",
    "- what is the variance between replicates? \n",
    "- How well do replicates correlate with one another?\n",
    "    - pearson between replicates? \n",
    "    - spearman between replicates? \n",
    "- what is the correlation across replicate means, medians within group?\n",
    "\n",
    "- is there are difference between log2 ratio and standard scaling activity? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ce4d86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:46.212521Z",
     "start_time": "2023-09-29T23:33:41.952650Z"
    }
   },
   "outputs": [],
   "source": [
    "import config_readwrite as crw\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multitest import fdrcorrection, multipletests\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "\n",
    "from plot_params import fonts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a792b",
   "metadata": {},
   "source": [
    "# Read, write to config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fde0cb5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:46.253543Z",
     "start_time": "2023-09-29T23:33:46.245380Z"
    }
   },
   "outputs": [],
   "source": [
    "# read\n",
    "config, cfn = crw.read(os.path.join(os.path.dirname(os.getcwd()), \"config.ini\"))\n",
    "\n",
    "# path\n",
    "DATA_PATH = config[\"local_path\"][\"data\"]\n",
    "\n",
    "# make dictionary of values to write to config\n",
    "config_dict = {\n",
    "    \"HEPG2\": os.path.join(DATA_PATH, \"full_hepg2_ultrasound_MPRA.csv\"),\n",
    "    \"HEPG2.clean\": os.path.join(DATA_PATH, \"full_hepg2_ultrasound_MPRA.clean.csv\"),\n",
    "    \"HEPG2.clean.transformed\": os.path.join(DATA_PATH, \"full_hepg2_ultrasound_MPRA.clean.transformed.csv\"),\n",
    "    \"HEPG2.clean.trans.scaled\": os.path.join(DATA_PATH, \"full_hepg2_ultrasound_MPRA.clean.transformed.standard.scaled.csv\"),\n",
    "    \"BJ\": os.path.join(DATA_PATH, \"full_bj_ultrasound_MPRA.csv\"),\n",
    "    \"BJ.clean\": os.path.join(DATA_PATH, \"full_bj_ultrasound_MPRA.clean.csv\"),\n",
    "    \"BJ.clean.transformed\": os.path.join(DATA_PATH, \"full_bj_ultrasound_MPRA.clean.transformed.csv\"),\n",
    "    \"BJ.clean.trans.scaled\": os.path.join(DATA_PATH, \"full_bj_ultrasound_MPRA.clean.transformed.standard.scaled.csv\"),\n",
    "    \"FASTA\":os.path.join(DATA_PATH, \"ultrasound_final_no_adapter.fasta\")\n",
    "}\n",
    "\n",
    "# make data section of config\n",
    "section = \"data\"\n",
    "crw.check(config, section)\n",
    "\n",
    "# add dictionary to config\n",
    "for key, value in config_dict.items():\n",
    "    config[section][key] = value\n",
    "    \n",
    "# write to config    \n",
    "crw.write(config, cfn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984a780",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71bcec27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:49.026183Z",
     "start_time": "2023-09-29T23:33:49.020768Z"
    }
   },
   "outputs": [],
   "source": [
    "def cohensd(test, ctrl):\n",
    "    cohens_d = (np.mean(test) - np.mean(ctrl)) / (np.sqrt((np.std(test) ** 2 + np.std(ctrl) ** 2) / 2))\n",
    "    return cohens_d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e62dd91",
   "metadata": {},
   "source": [
    "## mean, median, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5a0bee0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:49.559300Z",
     "start_time": "2023-09-29T23:33:49.553983Z"
    }
   },
   "outputs": [],
   "source": [
    "def computeStats(df, l2_ratios_list):\n",
    "    \"\"\" compute median, mean, std of ctrl and ultrasound replicates per sequence\"\"\"\n",
    "    \n",
    "    half_lst = int(len(l2_ratios_list)/2)\n",
    "    \n",
    "    print(\"ctrl cols:\", l2_ratios_list[:half_lst])\n",
    "    print(\"us cols:\", l2_ratios_list[half_lst:])\n",
    "    \n",
    "    df[\"l2.ratio.med.ctrl\"] = df[l2_ratios_list[:half_lst]].median(axis=1)\n",
    "    df[\"l2.ratio.mean.ctrl\"] = df[l2_ratios_list[:half_lst]].mean(axis=1)\n",
    "    df[\"l2.ratio.std.ctrl\"] = df[l2_ratios_list[:half_lst]].std(axis=1)\n",
    "\n",
    "    df[\"l2.ratio.med.us\"] = df[l2_ratios_list[half_lst:]].median(axis=1)\n",
    "    df[\"l2.ratio.mean.us\"] = df[l2_ratios_list[half_lst:]].mean(axis=1)\n",
    "    df[\"l2.ratio.std.us\"] = df[l2_ratios_list[half_lst:]].std(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec8285",
   "metadata": {},
   "source": [
    "## l2 transform (RNA/DNA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e794c47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:50.062557Z",
     "start_time": "2023-09-29T23:33:50.059365Z"
    }
   },
   "outputs": [],
   "source": [
    "def log2Transform(df, ratios_list):\n",
    "    \"\"\" log2 transform each ratio column\"\"\"\n",
    "    \n",
    "    for ratio in ratios_list:\n",
    "        df[f\"l2.{ratio}\"] = np.log2(df[ratio])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc3f065",
   "metadata": {},
   "source": [
    "## get coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9886accc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:50.597706Z",
     "start_time": "2023-09-29T23:33:50.593008Z"
    }
   },
   "outputs": [],
   "source": [
    "def getCoordinates(df):\n",
    "    \"\"\"str split name to get genomic coordinates for endogenous sequences\"\"\"\n",
    "    \n",
    "    coordf = df.loc[df['name'].str.contains(\"chr\")].copy()\n",
    "\n",
    "    coordf[\"coor\"] = coordf[\"name\"].apply(\n",
    "        lambda x: \"chr\"+x.split(\"chr\")[1] if \"chr\" in x else None)\n",
    "\n",
    "    # merge- add coordinates back to dataframe\n",
    "    return pd.merge(df, coordf, how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e5f18",
   "metadata": {},
   "source": [
    " ## compute fold change of medians "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68d2efd4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:51.115545Z",
     "start_time": "2023-09-29T23:33:51.111034Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def computeDelta(df):\n",
    "    \"\"\" compute delta of log2 median us - log2 median control\"\"\"\n",
    "    \n",
    "    df[\"delta.med\"] = df[\"l2.ratio.med.us\"]-df[\"l2.ratio.med.ctrl\"]\n",
    "    df[\"delta.mean\"] = df[\"l2.ratio.mean.us\"]-df[\"l2.ratio.mean.ctrl\"]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d851db09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:51.331281Z",
     "start_time": "2023-09-29T23:33:51.326927Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def clOrigin(df, constants_list):\n",
    "    \"\"\"annotate which cl a sequence was designed from\"\"\"\n",
    "    \n",
    "    df[\"cl.origin\"] = None\n",
    "\n",
    "    for cl in constants_list:\n",
    "        df.loc[df['name'].str.contains(cl), \"cl.origin\"] = cl\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f08ea62d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:51.647429Z",
     "start_time": "2023-09-29T23:33:51.643902Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def ctrlAnnot(df, constants_list):\n",
    "    \"\"\" annotate control type (pos, neg, test)\"\"\"\n",
    "    \n",
    "    df[\"type\"] = \"None\"\n",
    "    for ctrl in constants_list:\n",
    "        df.loc[df['label'].str.contains(ctrl), \"type\"] = ctrl\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87ee60e",
   "metadata": {},
   "source": [
    "## significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a3bf40c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:52.428454Z",
     "start_time": "2023-09-29T23:33:52.414044Z"
    },
    "code_folding": [
     24
    ]
   },
   "outputs": [],
   "source": [
    "def computeTTestPval(df, constants_list):\n",
    "    \"\"\" compute per sequence ttest of ctrl v. ultrasound rep\n",
    "        assume equal_var is False \n",
    "        \n",
    "        perform FDR correction per assay type - \n",
    "        'DEG', 'atac', 'k27ac', 'neg', 'pos', 'shuffle', 'synthetic'\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"pval\"] = None\n",
    "    for i, row in df.iterrows():\n",
    "        half_lst = int(len(constants_list)/2)\n",
    "        # get control replicates\n",
    "        ctrls = row[constants_list[:half_lst]]\n",
    "\n",
    "         # get US replicates\n",
    "        uss = row[constants_list[half_lst:]]\n",
    "        \n",
    "        # t-test per sequence, no equal variance assumed. \n",
    "        t,p = stats.ttest_ind(list(ctrls), list(uss), equal_var=False)\n",
    "        \n",
    "        # update dataframe\n",
    "        df.at[i,'pval'] = p\n",
    "    \n",
    "    return df\n",
    "\n",
    "def computeRepeatedMeasurePval(df, constants_list, peak_dict):\n",
    "    \"\"\"compute related T-Test for related measures of US v. control pvalue for all tiles tested in enhancer\n",
    "        controls for the independence of sequences. \n",
    "    \"\"\"\n",
    "    \n",
    "    test = df[constants_list + [\"name\", \"type\"]]\n",
    "\n",
    "    results ={}\n",
    "\n",
    "    for ENH in peak_dict.keys():\n",
    "\n",
    "        TILES = peak_dict[ENH]\n",
    "\n",
    "        t = test.loc[test[\"name\"].isin(TILES)].copy()\n",
    "\n",
    "        if len(t)>0:\n",
    "\n",
    "            tm = t.melt(id_vars=['name', \"type\"]) # melt data\n",
    "\n",
    "            tm[\"group\"] = tm[\"variable\"].apply(lambda x: x.split(\".\")[-1]) # make us and ctrl groups\n",
    "\n",
    "            s, t[\"pval.rep\"] = stats.ttest_rel(tm.loc[tm[\"group\"]==\"ctrl\", \"value\"], \n",
    "                              tm.loc[tm[\"group\"] !=\"ctrl\", \"value\"])\n",
    "            t[\"enh\"]=ENH\n",
    "            results[ENH] = t\n",
    "\n",
    "            del t, tm\n",
    "            \n",
    "    return pd.concat(results.values())\n",
    "\n",
    "def fdrcorr(df, pvalcol, alpha):\n",
    "    \"\"\"perform FDR correction per type of dataset\"\"\"\n",
    "    fdr_correction = {}\n",
    "\n",
    "    # FDR pvalue correction within dataset\n",
    "    for t in set(df[\"type\"]):\n",
    "        \n",
    "        # enhancer-wise correction\n",
    "        if \"enh\" in list(df):\n",
    "            test = df.loc[(df[\"type\"]==t), [\"enh\", pvalcol]].drop_duplicates().copy() \n",
    "            names = df.loc[df[\"type\"]==t, [\"enh\", \"name\"]].drop_duplicates().copy()\n",
    "        else:\n",
    "            test=df.loc[df[\"type\"]==t].copy()  # tile-wise correction\n",
    "\n",
    "        fs, fp, fsidak, fb = multipletests(test[pvalcol], \n",
    "                                            alpha=alpha, method=\"fdr_bh\"\n",
    "                                            )\n",
    "\n",
    "        bs, bp, sidak, bon = multipletests(test[pvalcol], \n",
    "                                            alpha=alpha, method=\"bonferroni\"\n",
    "                                            )\n",
    "\n",
    "        a_str = \"\".join(str(alpha).split(\".\"))\n",
    "        \n",
    "        \n",
    "        if \"enh\" in list(df):\n",
    "            test[f\"fdr.{a_str}.bool.rep\"], test[f\"fdr.{a_str}.pval.rep\"]= fs, fp\n",
    "\n",
    "            test[f\"bonf.{a_str}.bool.rep\"], test[f\"bonf.{a_str}.pval.rep\"] = bs, bp\n",
    "            fdr_correction[t] = pd.merge(names, test, how=\"left\")\n",
    "            del test, names\n",
    "            \n",
    "        else:\n",
    "            test[f\"fdr.{a_str}.bool\"], test[f\"fdr.{a_str}.pval\"]= fs, fp\n",
    "            test[f\"bonf.{a_str}.bool\"], test[f\"bonf.{a_str}.pval\"] = bs, bp\n",
    "            \n",
    "            fdr_correction[t] = test\n",
    "            del test\n",
    "        \n",
    "        \n",
    "    return pd.concat(fdr_correction.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e078d46f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:52.847769Z",
     "start_time": "2023-09-29T23:33:52.839102Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def bootstrap(data_list, size, stat):  \n",
    "    \n",
    "    \"\"\"\n",
    "    return the discrete and relative 95% confidence intervals of a data_list \n",
    "    \n",
    "    input\n",
    "        list of data (list, continuous values) - any list of data values, must be continuous values. \n",
    "        size (int) - size of dataset to bootstrap from list. If None, make bootstrapped distribution from entire list\n",
    "        stat (float or \"mean\") - quantile to bootstrap (float, 0-1) or mean (str)\n",
    "        \n",
    "    method \n",
    "        1. If size is None, get the length of the list \n",
    "        2. get the observed stat of the list (mean, median, quantile)\n",
    "        3. set bootstrap parameters\n",
    "        4. per iteration, randomly choose elements from the fold changes list w replacement\n",
    "        5. append the stat to the list of bootstrapped_stat\n",
    "        6. turn stats into a dataframe\n",
    "        7. calculate the delta distances from the population stat. This centers the data.\n",
    "        8. sort from largest to smallest difference\n",
    "        9. get discrete 0.025 adn 0.975 quantile values of the centered stat distribution. \n",
    "        10. calculate relative confidence intervals and actual confidence interval values (population stat - quantile values)\n",
    "    \"\"\"\n",
    "    \n",
    "    #1\n",
    "    if size is None:    \n",
    "        size = len(data_list) # size of distribution to bootstrap\n",
    "\n",
    "    #2\n",
    "    if type(stat) is float:\n",
    "        obs_stat = np.quantile(data_list, stat) # get observed stat\n",
    "        \n",
    "    elif stat==\"mean\":\n",
    "        obs_stat = np.mean(data_list) # get observed stat\n",
    "    \n",
    "    #3\n",
    "    nboot = 10000 # resample 10000 times\n",
    "    val = 0\n",
    "    bs_stats = []\n",
    "    \n",
    "    #4\n",
    "    while val < nboot:\n",
    "\n",
    "        bs_dist = np.random.choice(data_list, replace = True, size = size)\n",
    "        \n",
    "        #5\n",
    "        if type(stat) is float:\n",
    "            bs_stat = np.quantile(bs_dist, stat)\n",
    "        elif stat==\"mean\":\n",
    "            bs_stat = np.mean(bs_dist)\n",
    "            \n",
    "        bs_stats.append(bs_stat)\n",
    "        val +=1\n",
    "    #6\n",
    "    bs = pd.DataFrame(data = bs_stats, \n",
    "                      index = np.arange(nboot), \n",
    "                      columns = [\"bs_stat\"]) # make dataframe of bootstraps\n",
    "\n",
    "    #7 center the stat distribution\n",
    "    bs[\"deltas\"] = bs[\"bs_stat\"] - obs_stat\n",
    "\n",
    "    #8\n",
    "    bs = bs.sort_values(by = \"deltas\", ascending= False)\n",
    "    \n",
    "    #9  get discrete 95th CI\n",
    "    low = bs.deltas.quantile(0.025) \n",
    "    high = bs.deltas.quantile(0.975)\n",
    "    ci_relative = [high, low]  # assume obs value is centered at zero\n",
    "\n",
    "    #10  return ci relative to observed stat \n",
    "    ci_discrete = obs_stat - [high, low]  # assume obs value is center\n",
    "   \n",
    "    print(f\"measure CI of {stat} quantile| mean estimate\\n observed {stat} value:\", \n",
    "          obs_stat,  \n",
    "          \"\\ndiscrete diff from observed:\", ci_discrete, \n",
    "          \"\\nrelative diff from observed:\", ci_relative)\n",
    "    return ci_discrete, ci_relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "644c2cc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:53.180869Z",
     "start_time": "2023-09-29T23:33:53.175243Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def callActive(df):\n",
    "    \"\"\"call active elements from ctrl, us MPRA as values > 95 of shuffled regions\"\"\"\n",
    "    \n",
    "    # get shuffles\n",
    "    shufs = df.loc[df.name.str.contains(\"shuf\")].copy()\n",
    "    \n",
    "    # get 97.5% of shuffle median score for ctrl, US treatment\n",
    "    ctrl_975 = shufs['l2.ratio.med.ctrl'].quantile(0.975)\n",
    "    us_975 = shufs['l2.ratio.med.us'].quantile(0.975)\n",
    "    \n",
    "    \n",
    "    ctrl_025 = shufs['l2.ratio.med.ctrl'].quantile(0.025)\n",
    "    us_025 = shufs['l2.ratio.med.us'].quantile(0.025)\n",
    "    print(ctrl_975,ctrl_025, us_975, us_025)\n",
    "\n",
    "    # create column to label active\n",
    "    df['label.ctrl'], df['label.us']=False, False\n",
    "    \n",
    "    # label active elements - has more activity than 97.5% shuffles\n",
    "    df.loc[df['l2.ratio.med.ctrl']>ctrl_975, 'label.ctrl']=True\n",
    "    df.loc[df['l2.ratio.med.us']>us_975, 'label.us']=True\n",
    "\n",
    "    # label active elements - has less activity than 2.5% shuffles\n",
    "    df.loc[df['l2.ratio.med.ctrl']<ctrl_025, 'label.ctrl']=True\n",
    "    df.loc[df['l2.ratio.med.us']<us_025, 'label.us']=True\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ba5f25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:53.566722Z",
     "start_time": "2023-09-29T23:33:53.563334Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def callResponse(df):\n",
    "    df[\"response\"] = False\n",
    "    \n",
    "    df.loc[(df[\"pval\"]<0.005)&\n",
    "          (df['label.ctrl']!=df['label.us']),\n",
    "           \"response\"\n",
    "          ] = True\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1178146d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-28T01:20:05.600682Z",
     "start_time": "2023-09-28T01:20:05.597520Z"
    }
   },
   "source": [
    "# get enhancer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ddb2d8cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-29T23:33:58.063993Z",
     "start_time": "2023-09-29T23:33:54.373531Z"
    }
   },
   "outputs": [],
   "source": [
    "PEAKS = os.path.join(DATA_PATH, \"tiles.x.atac_k27ac_diff.bed\")\n",
    "p = pd.read_csv(PEAKS, sep='\\t')\n",
    "\n",
    "enh = {}\n",
    "for row in p.iterrows():\n",
    "    enh_id, name = row[1][\"enh_id\"], row[1][\"name\"]\n",
    "    if enh_id not in enh.keys():\n",
    "        enh[enh_id] = [name]\n",
    "        \n",
    "    elif enh_id in enh.keys():\n",
    "        name_lst = enh[enh_id]\n",
    "        name_lst.append(name)\n",
    "        enh[enh_id] = name_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d123d925",
   "metadata": {},
   "source": [
    "# test cell line \n",
    "\n",
    "**notes about this dataset**\n",
    "- Activity score - median score was assigned per US|CTRL for each sequence\n",
    "- Active | inactive = test sequence score > 95% shuffled score. Both these scores are median score (above)\n",
    "- logFC\n",
    "- P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58646021",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eaaf2eb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T00:05:43.296174Z",
     "start_time": "2023-09-30T00:05:43.290760Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "COMPUTE_TTEST = False\n",
    "COMPUTE_REP_TTEST = True\n",
    "COMPUTE_BS = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09845b6a",
   "metadata": {},
   "source": [
    "### constants dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "753d4569",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T00:05:43.992987Z",
     "start_time": "2023-09-30T00:05:43.983071Z"
    }
   },
   "outputs": [],
   "source": [
    "def constantsDict(n_reps):\n",
    "    if n_reps ==2:\n",
    "        l2ratios = ['l2.ratio.1.ctrl',\n",
    "                     #'l2.ratio.2.ctrl',\n",
    "                     'l2.ratio.3.ctrl', \n",
    "                      'l2.ratio.1.us', \n",
    "                     #'l2.ratio.2.us',\n",
    "                     'l2.ratio.3.us'\n",
    "                    ]\n",
    "    else:\n",
    "        l2ratios = ['l2.ratio.1.ctrl',\n",
    "                     'l2.ratio.2.ctrl',\n",
    "                     'l2.ratio.3.ctrl', \n",
    "                      'l2.ratio.1.us', \n",
    "                     'l2.ratio.2.us',\n",
    "                     'l2.ratio.3.us'\n",
    "                    ]\n",
    "    constants = {\n",
    "        \"NAMES\" :['name',\n",
    "             'label',\n",
    "             'ratio.med.ctrl',\n",
    "             'label.ctrl',\n",
    "             'ratio.1.ctrl',\n",
    "             'ratio.2.ctrl',\n",
    "             'ratio.3.ctrl',\n",
    "             'ratio.med.us',\n",
    "             'label.us',\n",
    "             'ratio.1.us',\n",
    "             'ratio.2.us',\n",
    "             'ratio.3.us',\n",
    "             'pval',\n",
    "             'logFC',\n",
    "             'response'],\n",
    "\n",
    "        \"CTRL_LIST\" : [\"neg\", \"pos\", \"shuffle\", \"synthetic\", 'k27ac', \"atac\", \"DEG\"],\n",
    "\n",
    "        \"RATIOS\": ['ratio.1.ctrl','ratio.2.ctrl', 'ratio.3.ctrl', \n",
    "              'ratio.1.us', 'ratio.2.us','ratio.3.us'],\n",
    "\n",
    "        \"L2RATIOS\": l2ratios,\n",
    "\n",
    "        \"CL_LIST\" :[\"k562\", \"hob\", \"bj\", \"hepg2\"]\n",
    "    }\n",
    "    return constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429c58ef",
   "metadata": {},
   "source": [
    "### input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062762fe",
   "metadata": {},
   "source": [
    "multiple colinearities - do group-wise difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9dcb2b6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T00:14:55.664658Z",
     "start_time": "2023-09-30T00:09:53.236211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ctrl cols: ['l2.ratio.1.ctrl', 'l2.ratio.3.ctrl']\n",
      "us cols: ['l2.ratio.1.us', 'l2.ratio.3.us']\n",
      "ctrl cols: ['l2.ratio.1.ctrl', 'l2.ratio.2.ctrl', 'l2.ratio.3.ctrl']\n",
      "us cols: ['l2.ratio.1.us', 'l2.ratio.2.us', 'l2.ratio.3.us']\n",
      "ctrl cols: ['l2.ratio.1.ctrl', 'l2.ratio.3.ctrl']\n",
      "us cols: ['l2.ratio.1.us', 'l2.ratio.3.us']\n",
      "ctrl cols: ['l2.ratio.1.ctrl', 'l2.ratio.2.ctrl', 'l2.ratio.3.ctrl']\n",
      "us cols: ['l2.ratio.1.us', 'l2.ratio.2.us', 'l2.ratio.3.us']\n"
     ]
    }
   ],
   "source": [
    "#CL = \"BJ\" #\"HEPG2\"\n",
    "results = {}\n",
    "for CL in [\"BJ\", \"HEPG2\"]:\n",
    "    DATA = config_dict[CL]\n",
    "    CLEAN = config_dict[f\"{CL}.clean.transformed\"]\n",
    "    SCALED =config_dict[f'{CL}.clean.trans.scaled']\n",
    "    \n",
    "    for N_REPS in [2,3]:\n",
    "        constants = constantsDict(N_REPS)\n",
    "    \n",
    "\n",
    "        ## LOAD\n",
    "\n",
    "        #if os.path.exists(CLEAN) is False:\n",
    "        # load .csv data, rename columns, skip row 1 (old column names)\n",
    "        df_ = pd.read_csv(DATA, skiprows=1, names=constants[\"NAMES\"], low_memory=False)\n",
    "\n",
    "        # annotate cell line origin\n",
    "        df = clOrigin(df_, constants[\"CL_LIST\"])\n",
    "\n",
    "        # annotate controls\n",
    "        df = ctrlAnnot(df,constants[\"CTRL_LIST\"])\n",
    "\n",
    "        # coordinates\n",
    "        df = getCoordinates(df)\n",
    "\n",
    "        # log2 transform ratios - increase sensitivity for ratios <1\n",
    "        df = log2Transform(df, constants[\"RATIOS\"])\n",
    "\n",
    "        # compute stats\n",
    "        df = computeStats(df, constants[\"L2RATIOS\"])\n",
    "\n",
    "        # compute difference between scaled\n",
    "        df = computeDelta(df)\n",
    "\n",
    "        # drop the ratio 2\n",
    "        #df = df.drop(columns=\"l2.ratio.2.ctrl\")\n",
    "\n",
    "        df.head()\n",
    "\n",
    "        ## compute T-test per tile\n",
    "\n",
    "\n",
    "        if COMPUTE_TTEST is True:\n",
    "            df = computeTTestPval(df, constants[\"L2RATIOS\"]) # slow 1:43\n",
    "\n",
    "            # fdr correction and bonferroni correction\n",
    "            pvalcol=\"pval\"\n",
    "            alpha=0.1\n",
    "            df = fdrcorr(df, pvalcol, alpha)\n",
    "\n",
    "            keep_cols = [\"name\", 'label', \n",
    "                         'cl.origin',\n",
    "                         'type',\n",
    "                         'coor',\n",
    "                         \"pval\", \n",
    "                         'fdr.01.bool',\n",
    "                         'fdr.01.pval',\n",
    "                         'bonf.01.bool',\n",
    "                         'bonf.01.pval',\n",
    "                         'delta.med',\n",
    "                        ]\n",
    "\n",
    "            keep = df[keep_cols].drop_duplicates()\n",
    "\n",
    "            out = os.path.join(DATA_PATH, f\"{CL}.sig.tiles.{N_REPS}rep.tsv\")\n",
    "            keep['CL'] = CL\n",
    "            keep.sort_values(by='fdr.01.pval').to_csv(out, sep='\\t', index=False)\n",
    "\n",
    "        ## repeated measures\n",
    "\n",
    "        if COMPUTE_REP_TTEST is True:\n",
    "            \n",
    "            out_filter = os.path.join(DATA_PATH, f\"{CL}.sig.rep.enh.{N_REPS}rep.p.lessthan5.tsv\")\n",
    "            out = os.path.join(DATA_PATH, f\"{CL}.sig.rep.enh.{N_REPS}rep.p.all.tsv\")\n",
    "            \n",
    "            if os.path.exists(out) is False:\n",
    "                # do this only for k27ac and atac. Remove any shuffled sequences from the stats\n",
    "                rep_df = computeRepeatedMeasurePval(df.loc[df[\"type\"].isin([\"k27ac\", \"atac\"])], \n",
    "                                                    constants[\"L2RATIOS\"], enh) # 1minute, 32 seconds\n",
    "\n",
    "                rep_df.head()\n",
    "\n",
    "                ## FDR correction of repeated measures\n",
    "\n",
    "                pvalcol=\"pval.rep\"\n",
    "                alpha=0.1\n",
    "                rep_df = fdrcorr(rep_df, pvalcol, alpha)\n",
    "\n",
    "                r = rep_df[[\"name\", \"enh\", \"pval.rep\", \"fdr.01.pval.rep\"]].drop_duplicates()\n",
    "                d = df[[ \"name\", \"delta.med\",    \n",
    "                        'l2.ratio.med.ctrl', 'l2.ratio.med.us','label.ctrl','label.us']].drop_duplicates()\n",
    "                \n",
    "                dr = pd.merge(d,r,  how = \"left\")  # more tiles\n",
    "                dr.to_csv(out, sep='\\t', index=False)\n",
    "\n",
    "                # filter for activity increases\n",
    "\n",
    "                dr_filter = dr.loc[(dr[\"pval.rep\"] < 0.05) &\n",
    "                                   (dr[\"label.ctrl\"]==\"inactive\")&\n",
    "                                   (dr[\"label.us\"]==\"active\")&\n",
    "                                   (dr[\"delta.med\"] > 1)\n",
    "                                   ].sort_values(by=[\"delta.med\", \"l2.ratio.med.ctrl\"], ascending=False)\n",
    "\n",
    "\n",
    "                dr_filter['CL'] = CL\n",
    "                dr_filter.to_csv(out_filter, sep='\\t', index=False)\n",
    "            else:\n",
    "                dr_filter = pd.read_csv(out_filter, sep='\\t')\n",
    "\n",
    "        ## bootstrap the 95% of the control distribution\n",
    "\n",
    "        if COMPUTE_BS is True:\n",
    "            out = os.path.join(DATA_PATH, f\"{CL}.sig.bs.95ci.{N_REPS}rep.tsv\")\n",
    "            if os.path.exists(out) is False:\n",
    "                disc975, rel = bootstrap(df[\"delta.med\"], None, 0.975)\n",
    "                df[\"ci.975.bs\"] = df[\"delta.med\"].apply(lambda x: True if x>max(disc975) else False)\n",
    "                disc025, rel = bootstrap(df[\"delta.med\"], None, 0.025)\n",
    "                df[\"ci.025.bs\"] = df[\"delta.med\"].apply(lambda x: True if x<min(disc025) else False)\n",
    "\n",
    "                keep_cols = [\"name\", 'label', \n",
    "                             'cl.origin',\n",
    "                             'type',\n",
    "                             'coor',\n",
    "                             'delta.med',\n",
    "                             'ci.975.bs',\n",
    "                             'ci.025.bs',\n",
    "                             'label.ctrl','label.us'\n",
    "                            ]\n",
    "\n",
    "                keep = df[keep_cols].drop_duplicates()\n",
    "\n",
    "                keep['CL'] = CL\n",
    "                keep.loc[(keep['ci.975.bs'] == True)|\n",
    "                         (keep['ci.025.bs'] == True)].to_csv(out, sep='\\t', index=False)\n",
    "            else:\n",
    "                keep = pd.read_csv(out, sep='\\t')\n",
    "        results[f\"{CL}-{N_REPS}\"] = (keep, dr_filter, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c6ff952",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T00:15:04.263044Z",
     "start_time": "2023-09-30T00:15:04.259774Z"
    }
   },
   "outputs": [],
   "source": [
    "h2k, h2d, h2f = results[\"HEPG2-2\"]\n",
    "h3k, h3d, h3f = results[\"HEPG2-3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "333137dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T00:15:16.172252Z",
     "start_time": "2023-09-30T00:15:16.165616Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 21\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hob_atac_up_chr14:24366690-24366959',\n",
       " 'hob_k27ac_down_chr16:87951064-87951333',\n",
       " 'hob_k27ac_down_chr1:81802226-81802495',\n",
       " 'hob_k27ac_down_chr20:64062550-64062819',\n",
       " 'hob_k27ac_down_chr5:322521-322790',\n",
       " 'hob_k27ac_down_chr9:14322306-14322575'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(set(h2d[\"name\"])), len(set(h3d[\"name\"])))\n",
    "set(h2d[\"name\"]).intersection(set(h3d[\"name\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "32daf45f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T00:26:49.705836Z",
     "start_time": "2023-09-30T00:26:49.595039Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CL, N_REPS= \"HEPG2\", 3\n",
    "h3k = pd.merge(h3k, df[[\"name\", 'label.ctrl','label.us']])\n",
    "h3k.to_csv(os.path.join(DATA_PATH, f\"{CL}.sig.bs.95ci.{N_REPS}rep.tsv\"), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "697524ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T00:27:00.413184Z",
     "start_time": "2023-09-30T00:27:00.337890Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CL, N_REPS= \"HEPG2\", 2\n",
    "h2k = pd.merge(h2k, df[[\"name\", 'label.ctrl','label.us']])\n",
    "h2k.to_csv(os.path.join(DATA_PATH, f\"{CL}.sig.bs.95ci.{N_REPS}rep.tsv\"), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b558e8c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-30T00:27:08.117603Z",
     "start_time": "2023-09-30T00:27:08.113504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HEPG2'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d7680a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mamba)",
   "language": "python",
   "name": "mamba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 122.85,
   "position": {
    "height": "144.85px",
    "left": "1142px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
