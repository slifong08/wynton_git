{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to train DeepSTARR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used packages and their version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GPU environment \n",
    "\n",
    "# conda create --name DeepSTARR python=3.7 tensorflow-gpu=1.14.0 keras-gpu=2.2.4\n",
    "# conda activate DeepSTARR\n",
    "# conda install numpy=1.16.2 pandas=0.25.3 matplotlib=3.1.1 ipykernel=5.4.3\n",
    "# pip install git+git://github.com/AvantiShri/shap.git@master\n",
    "# pip install 'h5py<3.0.0'\n",
    "# pip install deeplift==0.6.13.0\n",
    "# pip install keras-tuner==1.0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers import BatchNormalization, InputLayer, Input\n",
    "from keras import models\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "    \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('Neural_Network_DNA_Demo/')\n",
    "#from helper # from https://github.com/const-ae/Neural_Network_DNA_Demo\n",
    "import IOHelper, SequenceHelper \n",
    "\n",
    "import random\n",
    "random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/wynton/home/ahituv/fongsl/EMF/US/ml_emf/bin/classifier'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/wynton/home/ahituv/fongsl/.wget-hsts'. HSTS will be disabled.\n",
      "--2024-02-16 11:02:03--  https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Train.fa\n",
      "Resolving prox1 (prox1)... 172.26.1.6\n",
      "Connecting to prox1 (prox1)|172.26.1.6|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 116924268 (112M) [application/octet-stream]\n",
      "Saving to: ‘Sequences_Train.fa’\n",
      "\n",
      "Sequences_Train.fa  100%[===================>] 111.51M   282KB/s    in 5m 30s  \n",
      "\n",
      "2024-02-16 11:07:34 (346 KB/s) - ‘Sequences_Train.fa’ saved [116924268/116924268]\n",
      "\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/wynton/home/ahituv/fongsl/.wget-hsts'. HSTS will be disabled.\n",
      "--2024-02-16 11:07:34--  https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Val.fa\n",
      "Resolving prox1 (prox1)... 172.26.1.6\n",
      "Connecting to prox1 (prox1)|172.26.1.6|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 11760406 (11M) [application/octet-stream]\n",
      "Saving to: ‘Sequences_Val.fa’\n",
      "\n",
      "Sequences_Val.fa    100%[===================>]  11.21M   162KB/s    in 35s     \n",
      "\n",
      "2024-02-16 11:08:10 (326 KB/s) - ‘Sequences_Val.fa’ saved [11760406/11760406]\n",
      "\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/wynton/home/ahituv/fongsl/.wget-hsts'. HSTS will be disabled.\n",
      "--2024-02-16 11:08:10--  https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Test.fa\n",
      "Resolving prox1 (prox1)... 172.26.1.6\n",
      "Connecting to prox1 (prox1)|172.26.1.6|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 12025132 (11M) [application/octet-stream]\n",
      "Saving to: ‘Sequences_Test.fa’\n",
      "\n",
      "Sequences_Test.fa   100%[===================>]  11.47M   264KB/s    in 38s     \n",
      "\n",
      "2024-02-16 11:08:50 (306 KB/s) - ‘Sequences_Test.fa’ saved [12025132/12025132]\n",
      "\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/wynton/home/ahituv/fongsl/.wget-hsts'. HSTS will be disabled.\n",
      "--2024-02-16 11:08:50--  https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Train.txt\n",
      "Resolving prox1 (prox1)... 172.26.1.6\n",
      "Connecting to prox1 (prox1)|172.26.1.6|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 43682499 (42M) [text/plain]\n",
      "Saving to: ‘Sequences_activity_Train.txt’\n",
      "\n",
      "Sequences_activity_ 100%[===================>]  41.66M   165KB/s    in 2m 13s  \n",
      "\n",
      "2024-02-16 11:11:03 (321 KB/s) - ‘Sequences_activity_Train.txt’ saved [43682499/43682499]\n",
      "\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/wynton/home/ahituv/fongsl/.wget-hsts'. HSTS will be disabled.\n",
      "--2024-02-16 11:11:03--  https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Val.txt\n",
      "Resolving prox1 (prox1)... 172.26.1.6\n",
      "Connecting to prox1 (prox1)|172.26.1.6|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 4409299 (4.2M) [text/plain]\n",
      "Saving to: ‘Sequences_activity_Val.txt’\n",
      "\n",
      "Sequences_activity_ 100%[===================>]   4.20M   385KB/s    in 12s     \n",
      "\n",
      "2024-02-16 11:11:16 (370 KB/s) - ‘Sequences_activity_Val.txt’ saved [4409299/4409299]\n",
      "\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/wynton/home/ahituv/fongsl/.wget-hsts'. HSTS will be disabled.\n",
      "--2024-02-16 11:11:16--  https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Test.txt\n",
      "Resolving prox1 (prox1)... 172.26.1.6\n",
      "Connecting to prox1 (prox1)|172.26.1.6|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 4476205 (4.3M) [text/plain]\n",
      "Saving to: ‘Sequences_activity_Test.txt’\n",
      "\n",
      "Sequences_activity_ 100%[===================>]   4.27M   365KB/s    in 13s     \n",
      "\n",
      "2024-02-16 11:11:30 (340 KB/s) - ‘Sequences_activity_Test.txt’ saved [4476205/4476205]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FASTA files with DNA sequences of genomic regions from train/val/test sets\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Train.fa'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Val.fa'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Test.fa'\n",
    "\n",
    "# Files with developmental and housekeeping activity of genomic regions from train/val/test sets\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Train.txt'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Val.txt'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load sequences and enhancer activity\n",
    "def prepare_input(set):\n",
    "    # Convert sequences to one-hot encoding matrix\n",
    "    file_seq = str(\"Sequences_\" + set + \".fa\")\n",
    "    input_fasta_data_A = IOHelper.get_fastas_from_file(file_seq, uppercase=True)\n",
    "\n",
    "    # get length of first sequence\n",
    "    sequence_length = len(input_fasta_data_A.sequence.iloc[0])\n",
    "\n",
    "    # Convert sequence to one hot encoding matrix\n",
    "    seq_matrix_A = SequenceHelper.do_one_hot_encoding(input_fasta_data_A.sequence, sequence_length,\n",
    "                                                      SequenceHelper.parse_alpha_to_seq)\n",
    "    print(seq_matrix_A.shape)\n",
    "    \n",
    "    X = np.nan_to_num(seq_matrix_A) # Replace NaN with zero and infinity with large finite numbers\n",
    "    X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "    Activity = pd.read_table(\"Sequences_activity_\" + set + \".txt\")\n",
    "    Y_dev = Activity.Dev_log2_enrichment\n",
    "    Y_hk = Activity.Hk_log2_enrichment\n",
    "    Y = [Y_dev, Y_hk]\n",
    "    \n",
    "    print(set)\n",
    "\n",
    "    return input_fasta_data_A.sequence, seq_matrix_A, X_reshaped, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(402296, 249, 4)\n",
      "Train\n",
      "(40570, 249, 4)\n",
      "Val\n",
      "(41186, 249, 4)\n",
      "Test\n"
     ]
    }
   ],
   "source": [
    "# Data for train/val/test sets\n",
    "X_train_sequence, X_train_seq_matrix, X_train, Y_train = prepare_input(\"Train\")\n",
    "X_valid_sequence, X_valid_seq_matrix, X_valid, Y_valid = prepare_input(\"Val\")\n",
    "X_test_sequence, X_test_seq_matrix, X_test, Y_test = prepare_input(\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build DeepSTARR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Additional metrics\n",
    "from scipy.stats import spearmanr\n",
    "def Spearman(y_true, y_pred):\n",
    "     return ( tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32), \n",
    "                       tf.cast(y_true, tf.float32)], Tout = tf.float32) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /wynton/home/ahituv/fongsl/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /wynton/home/ahituv/fongsl/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /wynton/home/ahituv/fongsl/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /wynton/home/ahituv/fongsl/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /wynton/home/ahituv/fongsl/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /wynton/home/ahituv/fongsl/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /wynton/home/ahituv/fongsl/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 249, 4)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_1st (Conv1D)             (None, 249, 256)     7424        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 249, 256)     1024        Conv1D_1st[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 249, 256)     0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 124, 256)     0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_2 (Conv1D)               (None, 124, 60)      46140       max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 124, 60)      240         Conv1D_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 124, 60)      0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 62, 60)       0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_3 (Conv1D)               (None, 62, 60)       18060       max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 62, 60)       240         Conv1D_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 62, 60)       0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 31, 60)       0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Conv1D_4 (Conv1D)               (None, 31, 120)      21720       max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 31, 120)      480         Conv1D_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 31, 120)      0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 15, 120)      0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 1800)         0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "Dense_1 (Dense)                 (None, 256)          461056      flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 256)          1024        Dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 256)          0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 256)          0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Dense_2 (Dense)                 (None, 256)          65792       dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 256)          1024        Dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 256)          0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 256)          0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "Dense_Dev (Dense)               (None, 1)            257         dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Dense_Hk (Dense)                (None, 1)            257         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 624,738\n",
      "Trainable params: 622,722\n",
      "Non-trainable params: 2,016\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:18:28.981457: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-02-16 11:18:28.981488: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-02-16 11:18:28.981521: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (dev2): /proc/driver/nvidia/version does not exist\n",
      "2024-02-16 11:18:28.981949: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2024-02-16 11:18:29.074936: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2494220000 Hz\n",
      "2024-02-16 11:18:29.078547: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x310c730 executing computations on platform Host. Devices:\n",
      "2024-02-16 11:18:29.078779: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'batch_size': 128,\n",
       " 'epochs': 100,\n",
       " 'early_stop': 10,\n",
       " 'kernel_size1': 7,\n",
       " 'kernel_size2': 3,\n",
       " 'kernel_size3': 5,\n",
       " 'kernel_size4': 3,\n",
       " 'lr': 0.002,\n",
       " 'num_filters': 256,\n",
       " 'num_filters2': 60,\n",
       " 'num_filters3': 60,\n",
       " 'num_filters4': 120,\n",
       " 'n_conv_layer': 4,\n",
       " 'n_add_layer': 2,\n",
       " 'dropout_prob': 0.4,\n",
       " 'dense_neurons1': 256,\n",
       " 'dense_neurons2': 256,\n",
       " 'pad': 'same'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'batch_size': 128,\n",
    "          'epochs': 100,\n",
    "          'early_stop': 10,\n",
    "          'kernel_size1': 7,\n",
    "          'kernel_size2': 3,\n",
    "          'kernel_size3': 5,\n",
    "          'kernel_size4': 3,\n",
    "          'lr': 0.002,\n",
    "          'num_filters': 256,\n",
    "          'num_filters2': 60,\n",
    "          'num_filters3': 60,\n",
    "          'num_filters4': 120,\n",
    "          'n_conv_layer': 4,\n",
    "          'n_add_layer': 2,\n",
    "          'dropout_prob': 0.4,\n",
    "          'dense_neurons1': 256,\n",
    "          'dense_neurons2': 256,\n",
    "          'pad':'same'}\n",
    "\n",
    "def DeepSTARR(params=params):\n",
    "    \n",
    "    lr = params['lr']\n",
    "    dropout_prob = params['dropout_prob']\n",
    "    n_conv_layer = params['n_conv_layer']\n",
    "    n_add_layer = params['n_add_layer']\n",
    "    \n",
    "    # body\n",
    "    input = kl.Input(shape=(249, 4))\n",
    "    x = kl.Conv1D(params['num_filters'], kernel_size=params['kernel_size1'],\n",
    "                  padding=params['pad'],\n",
    "                  name='Conv1D_1st')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    for i in range(1, n_conv_layer):\n",
    "        x = kl.Conv1D(params['num_filters'+str(i+1)],\n",
    "                      kernel_size=params['kernel_size'+str(i+1)],\n",
    "                      padding=params['pad'],\n",
    "                      name=str('Conv1D_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # dense layers\n",
    "    for i in range(0, n_add_layer):\n",
    "        x = kl.Dense(params['dense_neurons'+str(i+1)],\n",
    "                     name=str('Dense_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "    bottleneck = x\n",
    "    \n",
    "    # heads per task (developmental and housekeeping enhancer activities)\n",
    "    tasks = ['Dev', 'Hk']\n",
    "    outputs = []\n",
    "    for task in tasks:\n",
    "        outputs.append(kl.Dense(1, activation='linear', name=str('Dense_' + task))(bottleneck))\n",
    "\n",
    "    model = keras.models.Model([input], outputs)\n",
    "    model.compile(keras.optimizers.Adam(lr=lr),\n",
    "                  loss=['mse', 'mse'], # loss\n",
    "                  loss_weights=[1, 1], # loss weigths to balance\n",
    "                  metrics=[Spearman]) # additional track metric\n",
    "\n",
    "    return model, params\n",
    "\n",
    "DeepSTARR()[0].summary()\n",
    "DeepSTARR()[1] # dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training DeepSTARR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(selected_model, X_train, Y_train, X_valid, Y_valid, params):\n",
    "\n",
    "    my_history=selected_model.fit(X_train, Y_train,\n",
    "                                  validation_data=(X_valid, Y_valid),\n",
    "                                  batch_size=params['batch_size'], epochs=params['epochs'],\n",
    "                                  callbacks=[EarlyStopping(patience=params['early_stop'], monitor=\"val_loss\", restore_best_weights=True),\n",
    "                                             History()])\n",
    "    \n",
    "    return selected_model, my_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /wynton/home/ahituv/fongsl/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 402296 samples, validate on 40570 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-16 11:25:11.674863: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26880/402296 [=>............................] - ETA: 4:17 - loss: 5.3997 - Dense_Dev_loss: 2.4434 - Dense_Hk_loss: 2.9563 - Dense_Dev_Spearman: 0.1827 - Dense_Hk_Spearman: 0.1464"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/scratch/fongsl/ipykernel_1403318/1203914995.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepSTARR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/scratch/fongsl/ipykernel_1403318/783053444.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(selected_model, X_train, Y_train, X_valid, Y_valid, params)\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                   callbacks=[EarlyStopping(patience=params['early_stop'], monitor=\"val_loss\", restore_best_weights=True),\n\u001b[0;32m----> 7\u001b[0;31m                                              History()])\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mselected_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmy_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/micromamba/envs/DeepSTARR/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_model, main_params = DeepSTARR()\n",
    "main_model, my_history = train(main_model, X_train, Y_train, X_valid, Y_valid, main_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model performance: mean squared error (MSE) and Pearson (PCC) and Spearman (SCC) correlation coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create functions\n",
    "def summary_statistics(X, Y, set, task):\n",
    "    pred = main_model.predict(X, batch_size=main_params['batch_size'])\n",
    "    if task ==\"Dev\":\n",
    "        i=0\n",
    "    if task ==\"Hk\":\n",
    "        i=1\n",
    "    print(set + ' MSE ' + task + ' = ' + str(\"{0:0.2f}\".format(mean_squared_error(Y, pred[i].squeeze()))))\n",
    "    print(set + ' PCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.pearsonr(Y, pred[i].squeeze())[0])))\n",
    "    print(set + ' SCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.spearmanr(Y, pred[i].squeeze())[0])))\n",
    "    \n",
    "# run for each set and enhancer type\n",
    "summary_statistics(X_train, Y_train[0], \"train\", \"Dev\")\n",
    "summary_statistics(X_train, Y_train[1], \"train\", \"Hk\")\n",
    "summary_statistics(X_valid, Y_valid[0], \"validation\", \"Dev\")\n",
    "summary_statistics(X_valid, Y_valid[1], \"validation\", \"Hk\")\n",
    "summary_statistics(X_test, Y_test[0], \"test\", \"Dev\")\n",
    "summary_statistics(X_test, Y_test[1], \"test\", \"Hk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"DeepSTARR\"\n",
    "\n",
    "model_json = main_model.to_json()\n",
    "with open('Model_' + model_name + '.json', \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "main_model.save_weights('Model_' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSTARR",
   "language": "python",
   "name": "deepstarr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
