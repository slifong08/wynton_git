{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1600a576-78db-457a-846f-02204fcf1497",
   "metadata": {},
   "source": [
    "# 20240215 \n",
    "## SF\n",
    "- adapting deepstarr architecture for US MPRA ATAC differentially accessible data.\n",
    "- goal: train a simple CNN classifier to distiguish US-accessible (1) from Ctrl-accessible (0)\n",
    "- Model would learn features of us-only open-chromatin sequences v. control-only open chromatin sequences.\n",
    "- I may need to reformulate this goal as there are other chromatin elements that might be sensitive to US exposure, but may already be accessible.\n",
    "    - As well, there may be many sequences that are only active in the control setting and retain accessibility during US exposure, but have repressed activity during ultrasound treatment\n",
    "\n",
    "### Notes\n",
    "\n",
    "- some issue with F1 score from torcheval.metrics package. May need to change.\n",
    "- I have stolen the parameters from the deepstarr model. Some of these parameters might need to change.\n",
    "- I have stolen some features from the legnet model, including the onecycler learning rate scheduler. In this case, the make learning rate will be the static learning rate used from deepSTARR. In application, the learning rate may be even smaller, but not larger than the learning rate parameter from the deepSTARR model.\n",
    "- I like the parameter dictionary as a way to specify values. I have added many key:value pairs to accomodate information for the US learning task, such as the sequence size, the number of channels (in and out), the poolsize (hard-coded in deepstarr), and the weight decay, which is a regularization parameter for the one-cycler learning rate scheduler used in legnet. \n",
    "\n",
    "\n",
    "https://github.com/bernardo-de-almeida/DeepSTARR/blob/main/DeepSTARR/DeepSTARR_training.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bd5b256-8a83-49dc-a639-19b4d135be64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T00:57:58.640945Z",
     "start_time": "2024-02-16T00:57:58.637952Z"
    }
   },
   "outputs": [],
   "source": [
    "#from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import  Dataset\n",
    "#from torcheval.metrics import classification#binary_f1_score\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b50207",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:00:48.609756Z",
     "start_time": "2024-02-16T01:00:48.607247Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/wynton/home/ahituv/fongsl/micromamba/envs/torch/lib/python3.11/site-packages/torcheval/metrics/classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfcf8ed2-711c-46d0-b1bd-d870e7b659ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dc627e7-fa5d-4a69-b902-e70d9bce9962",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:00:49.102066Z",
     "start_time": "2024-02-16T01:00:49.099111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "#    else \"mps\"\n",
    " #   if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d6a5a-827b-4a88-bdf1-a3b758307c90",
   "metadata": {},
   "source": [
    "# neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a80312ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:00:50.228025Z",
     "start_time": "2024-02-16T01:00:50.224415Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {'batch_size': 128,\n",
    "          'epochs': 100,\n",
    "          'early_stop': 10,\n",
    "          'kernel_size1': 7, # originally 7\n",
    "          'kernel_size2': 3, # 3\n",
    "          'kernel_size3': 5, # 5\n",
    "          'kernel_size4': 3, # 3\n",
    "          'lr': 0.002,\n",
    "          'num_filters1': 256, #SF changed from num_filters to num_filters1\n",
    "          'num_filters2': 60,\n",
    "          'num_filters3': 60,\n",
    "          'num_filters4': 120,\n",
    "          'n_conv_layer': 4,\n",
    "          'n_add_layer': 2,\n",
    "          'dropout_prob': 0.4,\n",
    "          'dense_neurons1': 256,\n",
    "          'dense_neurons2': 256,\n",
    "          'pad':'same', \n",
    "          'seq_size':271,  # SF added\n",
    "          'n_channels':4,  # SF added\n",
    "          'out_ch':1,  # SF added\n",
    "          'pool_size':2, # SF added\n",
    "          \"weight_decay\":0.01 # SFadded\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a32c09-6bfc-4b9b-9ee1-6e31be9308ce",
   "metadata": {},
   "source": [
    "## DeepSTARR in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3be4834f-4f6b-417d-9844-839c6702eae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sizes(model, input_tensor):\n",
    "    output = input_tensor\n",
    "    for m in model.children():\n",
    "        output = m(output)\n",
    "        print(m, output.shape)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "582085f2-0362-47da-a07a-036a11c62868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:09:06.124483Z",
     "start_time": "2024-02-16T01:09:06.112561Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (first_hidden): Sequential(\n",
      "    (0): Conv1d(4, 256, kernel_size=(7,), stride=(1,))\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (hidden): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv1d(256, 60, kernel_size=(3,), stride=(1,))\n",
      "      (1): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv1d(60, 60, kernel_size=(5,), stride=(1,))\n",
      "      (1): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv1d(60, 120, kernel_size=(3,), stride=(1,))\n",
      "      (1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (output): Sequential(\n",
      "    (0): Flatten(start_dim=1, end_dim=-1)\n",
      "    (1): Linear(in_features=120, out_features=256, bias=True)\n",
      "    (2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=256, bias=True)\n",
      "    (5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU()\n",
      "    (7): Linear(in_features=1, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "SF converted to pytorch from keras, tensorflow deepstarr model:\n",
    "https://github.com/bernardo-de-almeida/DeepSTARR/blob/main/DeepSTARR/DeepSTARR_training.ipynb\n",
    "\n",
    "def DeepSTARR(params=params):\n",
    "    \n",
    "    lr = params['lr']\n",
    "    dropout_prob = params['dropout_prob']\n",
    "    n_conv_layer = params['n_conv_layer']\n",
    "    n_add_layer = params['n_add_layer']\n",
    "    \n",
    "    # body\n",
    "    input = kl.Input(shape=(249, 4))\n",
    "    x = kl.Conv1D(params['num_filters'], kernel_size=params['kernel_size1'],\n",
    "                  padding=params['pad'],\n",
    "                  name='Conv1D_1st')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    for i in range(1, n_conv_layer):\n",
    "        x = kl.Conv1D(params['num_filters'+str(i+1)],\n",
    "                      kernel_size=params['kernel_size'+str(i+1)],\n",
    "                      padding=params['pad'],\n",
    "                      name=str('Conv1D_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # dense layers\n",
    "    for i in range(0, n_add_layer):\n",
    "        x = kl.Dense(params['dense_neurons'+str(i+1)],\n",
    "                     name=str('Dense_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "    bottleneck = x\n",
    "    \n",
    "    # heads per task (developmental and housekeeping enhancer activities)\n",
    "    tasks = ['Dev', 'Hk']\n",
    "    outputs = []\n",
    "    for task in tasks:\n",
    "        outputs.append(kl.Dense(1, activation='linear', name=str('Dense_' + task))(bottleneck))\n",
    "\n",
    "    model = keras.models.Model([input], outputs)\n",
    "    model.compile(keras.optimizers.Adam(lr=lr),\n",
    "                  loss=['mse', 'mse'], # loss\n",
    "                  loss_weights=[1, 1], # loss weigths to balance\n",
    "                  metrics=[Spearman]) # additional track metric\n",
    "\n",
    "    return model, params\n",
    "\"\"\"\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self,\n",
    "                 param_dict):  # added param dict\n",
    "        super().__init__()\n",
    "\n",
    "        # SF added\n",
    "        self.param_dict = param_dict\n",
    "        self.in_ch = self.param_dict[\"seq_size\"]*self.param_dict[\"n_channels\"]\n",
    "        self.in_ch = self.param_dict[\"n_channels\"]\n",
    "        self.out_ch = self.param_dict[\"out_ch\"]\n",
    "        \n",
    "\n",
    "        # first layer\n",
    "        self.first_hidden = nn.Sequential(\n",
    "            # in channels, out channels, kernel size\n",
    "            nn.Conv1d(\n",
    "                self.in_ch, self.param_dict[\"num_filters1\"], self.param_dict[\"kernel_size1\"]),\n",
    "            nn.BatchNorm1d(self.param_dict[\"num_filters1\"]),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(self.param_dict[\"pool_size\"])\n",
    "\n",
    "        )\n",
    "\n",
    "        # other layers\n",
    "        blocks = []\n",
    "\n",
    "        for i in range(1, self.param_dict[\"n_conv_layer\"]):\n",
    "            stack = nn.Sequential(\n",
    "                # in channels, out channels, kernel size\n",
    "                nn.Conv1d(\n",
    "                    self.param_dict[f\"num_filters{str(i)}\"],\n",
    "                    self.param_dict[f\"num_filters{str(i+1)}\"],\n",
    "                    self.param_dict[f\"kernel_size{str(i+1)}\"]),\n",
    "                nn.BatchNorm1d(self.param_dict[f\"num_filters{str(i+1)}\"]),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool1d(self.param_dict[\"pool_size\"])\n",
    "\n",
    "            )\n",
    "            blocks.append(stack)\n",
    "        self.hidden = nn.Sequential(*blocks)\n",
    "\n",
    "        fcs = []\n",
    "        # fully connected layers\n",
    "\n",
    "        self.output = nn.Sequential(\n",
    "                        nn.Flatten(),\n",
    "                        nn.Linear(self.param_dict[f\"num_filters{str(i+1)}\"],\n",
    "                                   self.param_dict['dense_neurons1']),\n",
    "                        nn.BatchNorm1d(self.param_dict['dense_neurons1']),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(self.param_dict['dense_neurons1'],\n",
    "                                   self.param_dict['dense_neurons2']),\n",
    "                        nn.BatchNorm1d(self.param_dict['dense_neurons2']),\n",
    "                        nn.ReLU(),\n",
    "                        \n",
    "                        nn.Linear(self.param_dict['out_ch'],\n",
    "                             self.param_dict['out_ch'])\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=self.first_hidden(x)\n",
    "        x=self.hidden(x)\n",
    "\n",
    "        x=self.output(x)\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork(params).to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bc4ba2dd-ebf1-45b9-a5ac-e6bf4cd9e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4, 7])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([60, 256, 3])\n",
      "torch.Size([60])\n",
      "torch.Size([60])\n",
      "torch.Size([60])\n",
      "torch.Size([60, 60, 5])\n",
      "torch.Size([60])\n",
      "torch.Size([60])\n",
      "torch.Size([60])\n",
      "torch.Size([120, 60, 3])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([120])\n",
      "torch.Size([256, 120])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256, 256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([256])\n",
      "torch.Size([1, 1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for i in model.parameters():\n",
    "    print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbac02f-36e0-430f-8527-6d970e643f86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# load data, make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfb46917-21e8-436f-abe0-bef4ee420ea0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:09:08.411488Z",
     "start_time": "2024-02-16T01:09:08.408804Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA=\"/wynton/group/ahituv/data/US-MPRA/ATAC-seq/hepg2.training.nojoint.class.tsv\"\n",
    "OUT_CH = 1\n",
    "val_fold=3\n",
    "test_fold=5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c6b817-89ad-47e6-b976-fc716e2011c0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# datamodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "caaeaace-b7a6-4521-8688-545c7dfccce2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:09:11.779508Z",
     "start_time": "2024-02-16T01:09:11.764618Z"
    }
   },
   "outputs": [],
   "source": [
    "# from datamodule\n",
    "# open the dataframe\n",
    "df = pd.read_csv(DATA,\n",
    "         sep='\\t', nrows=10)\n",
    "\n",
    "# rename columns - in parts\n",
    "seq_cols= ['seq_id', 'seq']\n",
    "mean_value_cols = [f\"mean_value{i+1}\" for i in np.arange(0,OUT_CH)]\n",
    "fold_rev_cols =  ['fold_num', 'rev']\n",
    "seq_cols+= mean_value_cols+fold_rev_cols\n",
    "df.columns = seq_cols[0:len(df.columns)]  # SF updated with second column\n",
    "\n",
    "\n",
    "if \"rev\" in df.columns:\n",
    "    df = df[df.rev == 0]\n",
    "    \n",
    "train = df[~df[\"fold_num\"].isin([val_fold, test_fold])]\n",
    "valid = df[df[\"fold_num\"] == val_fold]\n",
    "test = df[df[\"fold_num\"] == test_fold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190ca35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# one hot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf1fe3",
   "metadata": {},
   "source": [
    "## read fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c40eaf25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:07:50.246925Z",
     "start_time": "2024-02-16T01:07:50.243395Z"
    }
   },
   "outputs": [],
   "source": [
    "def makeDfFromFasta(fa_file):\n",
    "    \"\"\"SF: make pd dataframe from fasta file, \n",
    "        sequence is all uppercase\n",
    "        requires biopython\n",
    "    \n",
    "    \"\"\"\n",
    "    # parse fasta file\n",
    "    fa_dict = OrderedDict()  # dict to store names, and sequences\n",
    "    \n",
    "    with open(fa_file, \"r\") as reader:\n",
    "        for value in SimpleFastaParser(reader):\n",
    "            fa_dict[value[0]] = value[1].upper() # save sequences as upper case \n",
    "    \n",
    "    # make dataframe from fasta\n",
    "    fa_df = pd.concat(fa_dict.values())\n",
    "    \n",
    "    fa_df.columns = [\"seq_id\", \"seq\"]  # rename columns\n",
    "    \n",
    "    return fa_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330dca59",
   "metadata": {},
   "source": [
    "## one hot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc629b4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:07:51.057177Z",
     "start_time": "2024-02-16T01:07:51.051024Z"
    }
   },
   "outputs": [],
   "source": [
    "# Taken from https://github.com/const-ae/Neural_Network_DNA_Demo/blob/master/helper/SequenceHelper.py\n",
    "def to_categorical(y, nb_classes=None):\n",
    "    '''Convert class vector (integers from 0 to nb_classes)\n",
    "    to binary class matrix, for use with categorical_crossentropy\n",
    "    '''\n",
    "    y = np.asarray(y, dtype='int32')\n",
    "    if not nb_classes:\n",
    "        nb_classes = np.max(y) + 1\n",
    "    Y = np.zeros((len(y), nb_classes))\n",
    "    for i in range(len(y)):\n",
    "        if y[i] != -1:\n",
    "            Y[i, y[i]] = 1.\n",
    "    return Y\n",
    "\n",
    "\n",
    "def parse_alpha_to_seq(sequence):\n",
    "    \"\"\"replace nucleotides with values\"\"\"\n",
    "    \n",
    "    output = np.arange(len(sequence))\n",
    "    for i in range(0, len(sequence)):\n",
    "        snippet = sequence[i].upper()\n",
    "        if snippet == 'A':\n",
    "            output[i] = 0\n",
    "        elif snippet == 'C':\n",
    "            output[i] = 1\n",
    "        elif snippet == 'T':\n",
    "            output[i] = 2\n",
    "        elif snippet == 'G':\n",
    "            output[i] = 3\n",
    "        elif snippet == 'N':\n",
    "            output[i] = -1\n",
    "        else:\n",
    "            raise AssertionError(\"Cannot handle snippet: \" + snippet)\n",
    "    return output\n",
    "\n",
    "\n",
    "def do_one_hot_encoding(sequence, seq_length, f=parse_alpha_to_seq):\n",
    "    \n",
    "    # make an empty matrix of zeros\n",
    "    X = np.zeros((sequence.shape[0], seq_length, 4))\n",
    "\n",
    "    # one-hot encode each sequence\n",
    "    for idx in range(0, len(sequence)):\n",
    "        p = parse_alpha_to_seq(sequence.iloc[idx])\n",
    "        \n",
    "        # make matrix into long form, where each row is a nucleotide position, each cell is the identity. \n",
    "        X[idx]= to_categorical(p, 4)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b45a09c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## reading fastas, preparing datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bbb152d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:07:51.659552Z",
     "start_time": "2024-02-16T01:07:51.654291Z"
    }
   },
   "outputs": [],
   "source": [
    "# function to load sequences and enhancer activity\n",
    "def prepare_input(df=None, fa_file=None, label_file=None, label_df=None):\n",
    "    \n",
    "    \"\"\"Convert sequences to one-hot encoding matrix, \n",
    "        return sequences, matrix of sequences, and torch tensors x, y\n",
    "    \"\"\"\n",
    "    \n",
    "    #fa = str(\"Sequences_\" + set + \".fa\")\n",
    "    \n",
    "    if fa_file:\n",
    "        df = makeDfFromFasta(fa_file)\n",
    "\n",
    "    # get length of first sequence\n",
    "    seq_len = len(df[\"seq\"].iloc[0])\n",
    "\n",
    "    # Convert sequence to one hot encoding matrix\n",
    "    seq_matrix_A = do_one_hot_encoding(df[\"seq\"], seq_len,f = parse_alpha_to_seq)\n",
    "    # print(seq_matrix_A.shape)\n",
    "    \n",
    "    X = np.nan_to_num(seq_matrix_A) # Replace NaN with zero and infinity with large finite numbers\n",
    "    X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "    #\n",
    "    if label_file:\n",
    "        label_df = pd.read_csv(label_file, sep='\\t')\n",
    "    Y = label_df[label_df.columns[1:]]  # first column will be the sequence id\n",
    "\n",
    "    print('df.seq.shape', df[\"seq\"].shape, \"seq.matrix.shape\", seq_matrix_A.shape, \n",
    "          \"X_reshaped\", X_reshaped.shape, \"Y.shape\", Y.shape)\n",
    "    \n",
    "    return df[\"seq\"], seq_matrix_A, X_reshaped, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ddd06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Prepping actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54461f33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:07:52.341665Z",
     "start_time": "2024-02-16T01:07:52.264197Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.seq.shape (80,) seq.matrix.shape (80, 271, 4) X_reshaped (80, 271, 4) Y.shape (80, 1)\n",
      "df.seq.shape (12,) seq.matrix.shape (12, 271, 4) X_reshaped (12, 271, 4) Y.shape (12, 1)\n",
      "df.seq.shape (8,) seq.matrix.shape (8, 271, 4) X_reshaped (8, 271, 4) Y.shape (12, 1)\n"
     ]
    }
   ],
   "source": [
    "# from datamodule\n",
    "# open the dataframe\n",
    "df = pd.read_csv(DATA,\n",
    "                 sep='\\t', nrows=100)\n",
    "\n",
    "# rename columns - in parts\n",
    "seq_cols = ['seq_id', 'seq']\n",
    "mean_value_cols = [f\"mean_value{i+1}\" for i in np.arange(0, OUT_CH)]\n",
    "fold_rev_cols = ['fold_num', 'rev']\n",
    "seq_cols += mean_value_cols+fold_rev_cols\n",
    "df.columns = seq_cols[0:len(df.columns)]  # SF updated with second column\n",
    "\n",
    "\n",
    "# get train, test, split\n",
    "train = df[~df[\"fold_num\"].isin([val_fold, test_fold])]\n",
    "valid = df[df[\"fold_num\"] == val_fold]\n",
    "test = df[df[\"fold_num\"] == test_fold]\n",
    "\n",
    "\n",
    "label_df = train[[\"seq_id\", \"mean_value1\"]]\n",
    "X_train_sequence, X_train_seq_matrix, X_train, Y_train = prepare_input(train, None, None, label_df)\n",
    "label_df_v = valid[[\"seq_id\", \"mean_value1\"]]\n",
    "X_valid_sequence, X_valid_seq_matrix, X_valid, Y_valid  = prepare_input(valid, None, None, label_df_v)\n",
    "label_df_t = valid[[\"seq_id\", \"mean_value1\"]]\n",
    "X_test_sequence, X_test_seq_matrix, X_test, Y_test = prepare_input(test, None, None, label_df_t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be620a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# loss function, optimizer, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "590e9981",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:07:53.777646Z",
     "start_time": "2024-02-16T01:07:53.770717Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # changed from MSE\n",
    "f1_fn = f1_score.BinaryF1Score()\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=params['lr'], #  / 25,\n",
    "                              weight_decay=params['weight_decay']\n",
    "                             )\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, # type: ignore\n",
    "                                                max_lr=params['lr'],\n",
    "                                                three_phase=False, \n",
    "                                                total_steps=10, #self.trainer.estimated_stepping_batches, # type: ignore\n",
    "                                                pct_start=0.3,\n",
    "                                                cycle_momentum=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4be7885-1b21-4dea-a4d8-1af8197650b4",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7eeb45de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-16T01:09:17.154791Z",
     "start_time": "2024-02-16T01:09:17.049751Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x1680 and 256x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X), batch_size):\n\u001b[1;32m      7\u001b[0m     Xbatch \u001b[38;5;241m=\u001b[39m X[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous() \u001b[38;5;66;03m# transpose to [batchsize, n channels(4 nucleotides), seq_len (271)]\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model(Xbatch)\n\u001b[1;32m      9\u001b[0m     ybatch \u001b[38;5;241m=\u001b[39m y[i:i\u001b[38;5;241m+\u001b[39mbatch_size]\n\u001b[1;32m     10\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, ybatch)\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[39], line 120\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst_hidden(x)\n\u001b[1;32m    118\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden(x)\n\u001b[0;32m--> 120\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x1680 and 256x256)"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "batch_size = 10\n",
    "X = torch.from_numpy(X_train).float()\n",
    "y_pred = Y_train\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size].transpose(1,2).contiguous() # transpose to [batchsize, n channels(4 nucleotides), seq_len (271)]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9ca63f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv1d(4, 256, kernel_size=(7,), stride=(1,))\n",
      "  (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (2): ReLU()\n",
      "  (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ") torch.Size([10, 256, 132])\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (0): Conv1d(256, 60, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (1): Sequential(\n",
      "    (0): Conv1d(60, 60, kernel_size=(5,), stride=(1,))\n",
      "    (1): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (2): Sequential(\n",
      "    (0): Conv1d(60, 120, kernel_size=(3,), stride=(1,))\n",
      "    (1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU()\n",
      "    (3): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      ") torch.Size([10, 120, 14])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (10x1680 and 120x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m print_sizes(model, Xbatch)\n",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m, in \u001b[0;36mprint_sizes\u001b[0;34m(model, input_tensor)\u001b[0m\n\u001b[1;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m input_tensor\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m----> 4\u001b[0m     output \u001b[38;5;241m=\u001b[39m m(output)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(m, output\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/torch/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (10x1680 and 120x256)"
     ]
    }
   ],
   "source": [
    "print_sizes(model, Xbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a243f72-d0dc-4989-922c-1d912bbe425b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
