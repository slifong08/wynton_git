{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to train DeepSTARR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used packages and their version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GPU environment \n",
    "\n",
    "# conda create --name DeepSTARR python=3.7 tensorflow-gpu=1.14.0 keras-gpu=2.2.4\n",
    "# conda activate DeepSTARR\n",
    "# conda install numpy=1.16.2 pandas=0.25.3 matplotlib=3.1.1 ipykernel=5.4.3\n",
    "# pip install git+git://github.com/AvantiShri/shap.git@master\n",
    "# pip install 'h5py<3.0.0'\n",
    "# pip install deeplift==0.6.13.0\n",
    "# pip install keras-tuner==1.0.1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# FASTA files with DNA sequences of genomic regions from train/val/test sets\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Train.fa'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Val.fa'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_Test.fa'\n",
    "\n",
    "# Files with USelopmental and housekeeping activity of genomic regions from train/val/test sets\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Train.txt'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Val.txt'\n",
    "!wget 'https://data.starklab.org/almeida/DeepSTARR/Data/Sequences_activity_Test.txt'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T20:19:26.447997Z",
     "start_time": "2024-04-18T20:19:26.442970Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "import keras.layers as kl\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.layers.core import Dropout, Reshape, Dense, Activation, Flatten\n",
    "from keras.layers import BatchNormalization, InputLayer, Input\n",
    "from keras import models\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, History, ModelCheckpoint\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "import sys, os\n",
    "#sys.path.append('Neural_Network_DNA_Demo/')\n",
    "#from helper # from https://github.com/const-ae/Neural_Network_DNA_Demo\n",
    "import IOHelper, SequenceHelper \n",
    "\n",
    "import random\n",
    "random.seed(1234)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:29:26.147448Z",
     "start_time": "2024-04-18T19:29:26.142710Z"
    }
   },
   "outputs": [],
   "source": [
    "CL = \"hepg2\"\n",
    "prefix = f\"class.{CL}.balanced\"\n",
    "#f\"/wynton/home/ahituv/fongsl/EMF/US/ml_emf/data/deepstarr/deseq2/{CL}.reg.all\"\n",
    "data_path = os.path.join(\"/wynton/home/ahituv/fongsl/MPRA/agarwal_2023/\") \n",
    "pred_task=\"class\"\n",
    "standard_scaling=False\n",
    "n_pred_task=1\n",
    "\n",
    "\n",
    "os.chdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T19:34:28.334940Z",
     "start_time": "2024-04-18T19:34:28.321404Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dirname=prefix+\".model\"\n",
    "project_dir = os.path.join(data_path, model_dirname)\n",
    "if os.path.exists(project_dir) is False:\n",
    "    os.mkdir(project_dir)\n",
    "    \n",
    "\n",
    "model_name=\"DeepSTARR_ATAC\"\n",
    "params = {'batch_size': 64,\n",
    "          'epochs': 25, # 100\n",
    "          'early_stop': 10,\n",
    "          'kernel_size1': 7,\n",
    "          'kernel_size2': 3,\n",
    "          'kernel_size3': 5,\n",
    "          'kernel_size4': 3,\n",
    "          'lr': 0.002,\n",
    "          'num_filters': 256,\n",
    "          'num_filters2': 60,\n",
    "          'num_filters3': 60,\n",
    "          'num_filters4': 120,\n",
    "          'n_conv_layer': 4,\n",
    "          'n_add_layer': 2,\n",
    "          'dropout_prob': 0.4,\n",
    "          'dense_neurons1': 256,\n",
    "          'dense_neurons2': 256,\n",
    "          'pad':'same', \n",
    "          \"n_nucleotides\":4, \n",
    "          \"seq_len\":200,\n",
    "          \"pred_task\": pred_task, #\"class\", # or \"reg\"\n",
    "          \"n_pred_tasks\": int(n_pred_task), # 'US, CTRL, \n",
    "          \n",
    "         }\n",
    "\n",
    "# SF add params to paramdict\n",
    "params[\"prefix\"] = prefix\n",
    "params[\"data_path\"]=data_path\n",
    "params[\"project_path\"]=project_dir\n",
    "params[\"standard_scaling\"]=bool(standard_scaling)\n",
    "\n",
    "# write\n",
    "model_json_fn = os.path.join(project_dir, f'Model_{model_name}.{prefix}.json')\n",
    "model_weights_fn=os.path.join(project_dir, f'Model_{model_name}.{prefix}.h5')\n",
    "\n",
    "# SF add change directory\n",
    "os.chdir(data_path)\n",
    "\n",
    "# function to load sequences and enhancer activity\n",
    "def prepare_input(set, prefix, scale=False):\n",
    "    # Convert sequences to one-hot encoding matrix\n",
    "    file_seq = str(f\"{prefix}.Sequences_\" + set + \".fa\")\n",
    "    input_fasta_data_A = IOHelper.get_fastas_from_file(file_seq, uppercase=True)\n",
    "\n",
    "    # get length of first sequence\n",
    "    sequence_length = len(input_fasta_data_A.sequence.iloc[0])\n",
    "\n",
    "    # Convert sequence to one hot encoding matrix\n",
    "    seq_matrix_A = SequenceHelper.do_one_hot_encoding(input_fasta_data_A.sequence, sequence_length,\n",
    "                                                      SequenceHelper.parse_alpha_to_seq)\n",
    "    print(seq_matrix_A.shape)\n",
    "    \n",
    "    X = np.nan_to_num(seq_matrix_A) # Replace NaN with zero and infinity with large finite numbers\n",
    "    X_reshaped = X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
    "\n",
    "    Activity = pd.read_table(f\"{prefix}.Sequences_activity_\" + set + \".txt\")\n",
    "    col_names = Activity.columns[1:]\n",
    "    \n",
    "    if scale is True:\n",
    "        print(\"\\n\\nSCALING DATA\\n\\n\")\n",
    "        # SCALE DATA\n",
    "        sc = StandardScaler()        \n",
    "\n",
    "        # standard scale activity columns\n",
    "        Y_sc = pd.DataFrame(sc.fit_transform(Activity[Activity.columns[1:]])) # don't transform first column w names\n",
    "        \n",
    "    else: \n",
    "        Y_sc = Activity[Activity.columns[1:]]\n",
    "        \n",
    "    Y = []\n",
    "\n",
    "    for i in Y_sc.columns:\n",
    "        Y.append(Y_sc[i])\n",
    "    \n",
    "    print(set)\n",
    "\n",
    "    if set ==\"Train\":\n",
    "        return input_fasta_data_A.sequence, seq_matrix_A, X_reshaped, Y, col_names, sequence_length\n",
    "    else:\n",
    "        return input_fasta_data_A.sequence, seq_matrix_A, X_reshaped, Y\n",
    "\n",
    "### Additional metrics\n",
    "\n",
    "def Spearman(y_true, y_pred):\n",
    "     return ( tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32), \n",
    "                       tf.cast(y_true, tf.float32)], Tout = tf.float32) )\n",
    "\n",
    "    \n",
    "def train(selected_model, X_train, Y_train, X_valid, Y_valid, params):\n",
    "\n",
    "    my_history=selected_model.fit(X_train, Y_train,\n",
    "                                  validation_data=(X_valid, Y_valid),\n",
    "                                  batch_size=params['batch_size'], \n",
    "                                  epochs=params['epochs'],\n",
    "                                  callbacks=[EarlyStopping(patience=params['early_stop'], \n",
    "                                                           monitor=\"val_loss\", \n",
    "                                                           restore_best_weights=True),\n",
    "                                             History()]\n",
    "                                 )\n",
    "    \n",
    "    return selected_model, my_history\n",
    "    \n",
    "\n",
    "\n",
    "# create functions\n",
    "\n",
    "\n",
    "def DeepSTARR(col_names, params=params):\n",
    "    \n",
    "    lr = params['lr']\n",
    "    dropout_prob = params['dropout_prob']\n",
    "    n_conv_layer = params['n_conv_layer']\n",
    "    n_add_layer = params['n_add_layer']\n",
    "    \n",
    "    # body\n",
    "    input = kl.Input(shape=(params['seq_len'], params['n_nucleotides']))\n",
    "    x = kl.Conv1D(params['num_filters'], kernel_size=params['kernel_size1'],\n",
    "                  padding=params['pad'],\n",
    "                  name='Conv1D_1st')(input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPooling1D(2)(x)\n",
    "\n",
    "    for i in range(1, n_conv_layer):\n",
    "        x = kl.Conv1D(params['num_filters'+str(i+1)],\n",
    "                      kernel_size=params['kernel_size'+str(i+1)],\n",
    "                      padding=params['pad'],\n",
    "                      name=str('Conv1D_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = MaxPooling1D(2)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # dense layers\n",
    "    for i in range(0, n_add_layer):\n",
    "        x = kl.Dense(params['dense_neurons'+str(i+1)],\n",
    "                     name=str('Dense_'+str(i+1)))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "    bottleneck = x\n",
    "    \n",
    "    # heads per task (developmental and housekeeping enhancer activities)\n",
    "    \n",
    "    # SF added below. Accommodate linear and classification tasks.\n",
    "    pred_task = params[\"pred_task\"] \n",
    "    \n",
    "    if pred_task == \"reg\":\n",
    "        activation_ = \"linear\"\n",
    "        loss_ = ['mse']*params['n_pred_tasks']\n",
    "        metrics_ = Spearman\n",
    "        \n",
    "    elif pred_task == \"class\":\n",
    "        if params['n_pred_tasks'] == 1:\n",
    "            activation_ = \"sigmoid\"  # use 'softmax' if binary #tf.nn.softmax\n",
    "        else:\n",
    "            activation_ = \"softmax\"  # use 'softmax' if binary #tf.nn.softmax\n",
    "        loss_ = ['binary_crossentropy']*params['n_pred_tasks']\n",
    "        metrics_ = 'accuracy'\n",
    "        \n",
    "    print(pred_task, activation_, loss_, metrics_)\n",
    "        \n",
    "    # end SF additions\n",
    "    \n",
    "    tasks = col_names  # for naming\n",
    "    outputs = []\n",
    "    for task in tasks:\n",
    "        outputs.append(kl.Dense(1, activation=activation_, name=str('Dense_' + task))(bottleneck))  # changed activation=\"linear\"\n",
    "\n",
    "    model = keras.models.Model([input], outputs)\n",
    "    model.compile(keras.optimizers.Adam(lr=lr),\n",
    "                  loss = loss_,  # SF changed loss=['mse', 'mse'], # loss\n",
    "                  loss_weights=[1]*params['n_pred_tasks'], # loss weigths to balance\n",
    "                  metrics=[metrics_]) # additional track metric\n",
    "\n",
    "    return model, params\n",
    "\n",
    "\n",
    "def summary_statistics(X, Y, set_var, task, i, params=params):\n",
    "    #X, Y, set_var, task, i = X_train, Y_train, \"train\", task, i\n",
    "    \n",
    "    # make prediction\n",
    "    pred = main_model.predict(X, batch_size=main_params['batch_size'])\n",
    "\n",
    "    if main_params['pred_task'] == \"reg\":\n",
    "        print(set_var +' MSE ' + task + ' = ' + \"{0:0.2f}\".format(mean_squared_error(Y[i], pred[i].squeeze())))\n",
    "        print(set_var + ' PCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.pearsonr(Y[i], pred[i].squeeze())[0])))\n",
    "        print(set_var + ' SCC ' + task + ' = ' + str(\"{0:0.2f}\".format(stats.spearmanr(Y[i], pred[i].squeeze())[0])))\n",
    "\n",
    "    else:  # handle classification \n",
    "        if params[\"n_pred_tasks\"] == 1:\n",
    "            pred_loss, pred_acc = main_model.evaluate(X, Y, batch_size=params[\"batch_size\"])\n",
    "            #pred_f1 = f1_score(Y[i], pred[:,i])\n",
    "        else:\n",
    "            pred_loss, pred_acc = main_model.evaluate(X, Y, batch_size=params[\"batch_size\"])\n",
    "            #pred_f1 = f1_score(Y[i], pred[i].squeeze())\n",
    "\n",
    "        print(set_var, \"accuracy \" + task + \" = \" + str(\"{0:0.2f}\".format(pred_acc)))\n",
    "        #print(set_var, \"f1 \" + task + \" = \" + str(\"{0:0.2f}\".format(pred_f1)))\n",
    "\n",
    "    return pred\n",
    "    \n",
    "\n",
    "### MAIN ###\n",
    "\n",
    "# Data for train/val/test sets\n",
    "X_train_sequence, X_train_seq_matrix, X_train, Y_train, col_names, seq_len = prepare_input(\"Train\", prefix, params[\"standard_scaling\"])\n",
    "X_valid_sequence, X_valid_seq_matrix, X_valid, Y_valid = prepare_input(\"Val\", prefix, params[\"standard_scaling\"])\n",
    "X_test_sequence, X_test_seq_matrix, X_test, Y_test = prepare_input(\"Test\", prefix, params[\"standard_scaling\"])\n",
    "\n",
    "params[\"n_pred_tasks\"] = len(col_names)  #update dictionary with actual N pred tasks based on col name\n",
    "params[\"seq_len\"] = seq_len\n",
    "\n",
    "#DeepSTARR(col_names)[0].summary()\n",
    "#DeepSTARR(col_names)[1] # dictionary\n",
    "\n",
    "main_model, main_params = DeepSTARR(col_names, params)\n",
    "main_model, my_history = train(main_model, X_train, Y_train, X_valid, Y_valid, main_params)\n",
    "\n",
    "\n",
    "# write to project dir\n",
    "os.chdir(project_dir)\n",
    "\n",
    "# write model\n",
    "\n",
    "## config\n",
    "model_json = main_model.to_json()\n",
    "with open(model_json_fn, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "### weights \n",
    "main_model.save_weights(model_weights_fn)\n",
    "\n",
    "# write params # SF added\n",
    "with open(f'config' + model_name + f'.{prefix}.json', \"w\") as json_file:\n",
    "    for key, value in params.items():\n",
    "        json_file.write(f\"{key}:{value}\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "main_model, main_params  = DeepSTARR(col_names, params)\n",
    "main_model.load_weights(filepath=model_weights, skip_mismatch=False)\n",
    "\n",
    "# plot training history\n",
    "\n",
    "pd.DataFrame(my_history.history).plot(figsize=(5,5))\n",
    "plt.gca().set_ylim(0, 1) \n",
    "plt.xlabel(\"epochs\")\n",
    "plt.savefig(\"history.png\", bbox_inches='tight')\n",
    "\n",
    "# run for each set and enhancer type. Changed to for-loop for ease.\n",
    "pred_names = [\"Y\"]  # collect list for\n",
    "\n",
    "# run predictions for validation, test\n",
    "\n",
    "train_pred = summary_statistics(X_train, Y_train, \"train\", task, i)\n",
    "val_pred = summary_statistics(X_valid, Y_valid, \"validation\", task, i)\n",
    "test_pred = summary_statistics(X_test, Y_test, \"test\", task, i)\n",
    "\n",
    "pred_names.append(f\"pred_{i}\")  # add to the list of pred_names\n",
    "\n",
    "Ydf = pd.DataFrame(Y_test).T\n",
    "Ydf.columns = [\"Y\"]\n",
    "\n",
    "pred_df = pd.DataFrame(test_pred, columns=list(col_names))\n",
    "preds = pd.merge(Ydf, pred_df, left_index=True, right_index=True)\n",
    "\n",
    "# write test predicted and observed values to file\n",
    "preds.to_csv(f\"{model_name}.{prefix}.test.predictions.tsv\", sep='\\t', index=False)    \n",
    "\n",
    "if \"class\" in prefix:\n",
    "    # plot prediction values. \n",
    "    fig, ax=plt.subplots(figsize=(4,4))\n",
    "    sns.boxplot(x=\"Y\", y=\"class\", data=preds.round(2))\n",
    "    ax.set(xlabel=\"test\", \n",
    "          ylabel=\"pred_class\")\n",
    "    plt.savefig('model_test.png', bbox_inches='tight')\n",
    "\n",
    "    # write descriptive stats\n",
    "    preds.groupby(\"Y\").describe().T.to_csv(\"model_test_stats.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-18T23:19:52.854250Z",
     "start_time": "2024-04-18T23:19:52.834381Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"8\" valign=\"top\">class</th>\n",
       "      <th>count</th>\n",
       "      <td>5208.000000</td>\n",
       "      <td>383.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.033632</td>\n",
       "      <td>0.074861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.052766</td>\n",
       "      <td>0.088937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000105</td>\n",
       "      <td>0.000886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.006257</td>\n",
       "      <td>0.016042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.015532</td>\n",
       "      <td>0.042135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.038556</td>\n",
       "      <td>0.097923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.728028</td>\n",
       "      <td>0.633054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Y                      0           1\n",
       "class count  5208.000000  383.000000\n",
       "      mean      0.033632    0.074861\n",
       "      std       0.052766    0.088937\n",
       "      min       0.000105    0.000886\n",
       "      25%       0.006257    0.016042\n",
       "      50%       0.015532    0.042135\n",
       "      75%       0.038556    0.097923\n",
       "      max       0.728028    0.633054"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepSTARR",
   "language": "python",
   "name": "deepstarr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
