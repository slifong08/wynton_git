{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "759f1daa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T00:21:57.634357Z",
     "start_time": "2023-10-25T00:21:44.586712Z"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import ClassVar, Iterator, Sequence\n",
    "\n",
    "import mmh3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ignite.contrib.handlers.tensorboard_logger import TensorboardLogger\n",
    "from ignite.contrib.handlers.tqdm_logger import ProgressBar\n",
    "from ignite.engine import Engine, Events\n",
    "from ignite.metrics import MeanSquaredError, Metric\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader, Dataset, Sampler\n",
    "\n",
    "from model import SeqNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f60d34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T00:22:00.469018Z",
     "start_time": "2023-10-25T00:22:00.463852Z"
    }
   },
   "outputs": [],
   "source": [
    "LEFT_ADAPTER = \"TGCATTTTTTTCACATC\"\n",
    "RIGHT_ADAPTER = \"GGTTACGGCTGTT\"\n",
    "\n",
    "PLASMID = \"aactctcaaggatcttaccgctgttgagatccagttcgatgtaacccactcgtgcacccaactgatcttcagcatcttttactttcaccagcgtttctgggtgagcaaaaacaggaaggcaaaatgccgcaaaaaagggaataagggcgacacggaaatgttgaatactcatactcttcctttttcaatattattgaagcatttatcagggttattgtctcatgagcggatacatatttgaatgtatttagaaaaataaacaaataggggttccgcgcacatttccccgaaaagtgccacctgacgtcatctatattaccctgttatccctagcggatctgccggtagaggtgtggtcaataagagcgacctcatactatacctgagaaagcaacctgacctacaggaaagagttactcaagaataagaattttcgttttaaaacctaagagtcactttaaaatttgtatacacttattttttttataacttatttaataataaaaatcataaatcataagaaattcgcttatttagaagtGGCGCGCCGGTCCGttacttgtacagctcgtccatgccgccggtggagtggcggccctcggcgcgttcgtactgttccacgatggtgtagtcctcgttgtgggaggtgatgtccaacttgatgttgacgttgtaggcgccgggcagctgcacgggcttcttggccttgtaggtggtcttgacctcagcgtcgtagtggccgccgtccttcagcttcagcctctgcttgatctcgcccttcagggcgccgtcctcggggtacatccgctcggaggaggcctcccagcccatggtcttcttctgcattacggggccgtcggaggggaagttggtgccgcgcagcttcaccttgtagatgaactcgccgtcctgcagggaggagtcctgggtcacggtcaccacgccgccgtcctcgaagttcatcacgcgctcccacttgaagccctcggggaaggacagcttcaagtagtcggggatgtcggcggggtgcttcacgtaggccttggagccgtacatgaactgaggggacaggatgtcccaggcgaagggcagggggccacccttggtcaccttcagcttggcggtctgggtgccctcgtaggggcggccctcgccctcgccctcgatctcgaactcgtggccgttcacggagccctccatgtgcaccttgaagcgcatgaactccttgatgatggccatgttatcctcctcgcccttgctcacCATGGTACTAGTGTTTAGTTAATTATAGTTCGTTGACCGTATATTCTAAAAACAAGTACTCCTTAAAAAAAAACCTTGAAGGGAATAAACAAGTAGAATAGATAGAGAGAAAAATAGAAAATGCAAGAGAATTTATATATTAGAAAGAGAGAAAGAAAAATGGAAAAAAAAAAATAGGAAAAGCCAGAAATAGCACTAGAAGGAGCGACACCAGAAAAGAAGGTGATGGAACCAATTTAGCTATATATAGTTAACTACCGGCTCGATCATCTCTGCCTCCAGCATAGTCGAAGAAGAATTTTTTTTTTCTTGAGGCTTCTGTCAGCAACTCGTATTTTTTCTTTCTTTTTTGGTGAGCCTAAAAAGTTCCCACGTTCTCTTGTACGACGCCGTCACAAACAACCTTATGGGTAATTTGTCGCGGTCTGGGTGTATAAATGTGTGGGTGCAACATGAATGTACGGAGGTAGTTTGCTGATTGGCGGTCTATAGATACCTTGGTTATGGCGCCCTCACAGCCGGCAGGGGAAGCGCCTACGCTTGACATCTACTATATGTAAGTATACGGCCCCATATATAggccctttcgtctcgcgcgtttcggtgatgacggtgaaaacctctgacacatgcagctcccggagacggtcacagcttgtctgtaagcggatgccgggagcagacaagcccgtcagggcgcgtcagcgggtgttggcgggtgtcggggctggcttaactatgcggcatcagagcagattgtactgagagtgcaccatatggacatattgtcgttagaacgcggctacaattaatacataaccttatgtatcatacacatacgatttaggtgacactatagaacgcggccgccagctgaagctttaactatgcggcatcagagcagattgtactgagagtgcaccataccaccttttcaattcatcattttttttttattcttttttttgatttcggtttccttgaaatttttttgattcggtaatctccgaacagaaggaagaacgaaggaaggagcacagacttagattggtatatatacgcatatgtagtgttgaagaaacatgaaattgcccagtattcttaacccaactgcacagaacaaaaacctgcaggaaacgaagataaatcatgtcgaaagctacatataaggaacgtgctgctactcatcctagtcctgttgctgccaagctatttaatatcatgcacgaaaagcaaacaaacttgtgtgcttcattggatgttcgtaccaccaaggaattactggagttagttgaagcattaggtcccaaaatttgtttactaaaaacacatgtggatatcttgactgatttttccatggagggcacagttaagccgctaaaggcattatccgccaagtacaattttttactcttcgaagacagaaaatttgctgacattggtaatacagtcaaattgcagtactctgcgggtgtatacagaatagcagaatgggcagacattacgaatgcacacggtgtggtgggcccaggtattgttagcggtttgaagcaggcggcagaagaagtaacaaaggaacctagaggccttttgatgttagcagaattgtcatgcaagggctccctatctactggagaatatactaagggtactgttgacattgcgaagagcgacaaagattttgttatcggctttattgctcaaagagacatgggtggaagagatgaaggttacgattggttgattatgacacccggtgtgggtttagatgacaagggagacgcattgggtcaacagtatagaaccgtggatgatgtggtctctacaggatctgacattattattgttggaagaggactatttgcaaagggaagggatgctaaggtagagggtgaacgttacagaaaagcaggctgggaagcatatttgagaagatgcggccagcaaaactaaaaaactgtattataagtaaatgcatgtatactaaactcacaaattagagcttcaatttaattatatcagttattaccctatgcggtgtgaaataccgcacagatgcgtaaggagaaaataccgcatcaggaaattgtaagcgttaatattttgttaaaattcgcgttaaatttttgttaaatcagctcattttttaaccaataggccgaaatcggcaaaatcccttataaatcaaaagaatagaccgagatagggttgagtgttgttccagtttggaacaagagtccactattaaagaacgtggactccaacgtcaaagggcgaaaaaccgtctatcagggcgatggcccactacgtgaaccatcaccctaatcaagtGCTAGCAGGAATGATGCAAAAGGTTCCCGATTCGAACTGCATTTTTTTCACATCNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNGGTTACGGCTGTTTCTTAATTAAAAAAAGATAGAAAACATTAGGAGTGTAACACAAGACTTTCGGATCCTGAGCAGGCAAGATAAACGAAGGCAAAGatgtctaaaggtgaagaattattcactggtgttgtcccaattttggttgaattagatggtgatgttaatggtcacaaattttctgtctccggtgaaggtgaaggtgatgctacttacggtaaattgaccttaaaattgatttgtactactggtaaattgccagttccatggccaaccttagtcactactttaggttatggtttgcaatgttttgctagatacccagatcatatgaaacaacatgactttttcaagtctgccatgccagaaggttatgttcaagaaagaactatttttttcaaagatgacggtaactacaagaccagagctgaagtcaagtttgaaggtgataccttagttaatagaatcgaattaaaaggtattgattttaaagaagatggtaacattttaggtcacaaattggaatacaactataactctcacaatgtttacatcactgctgacaaacaaaagaatggtatcaaagctaacttcaaaattagacacaacattgaagatggtggtgttcaattagctgaccattatcaacaaaatactccaattggtgatggtccagtcttgttaccagacaaccattacttatcctatcaatctgccttatccaaagatccaaacgaaaagagagaccacatggtcttgttagaatttgttactgctgctggtattacccatggtatggatgaattgtacaaataaggcgcgccacttctaaataagcgaatttcttatgatttatgatttttattattaaataagttataaaaaaaataagtgtatacaaattttaaagtgactcttaggttttaaaacgaaaattcttattcttgagtaactctttcctgtaggtcaggttgctttctcaggtatagtatgaggtcgctcttattgaccacacctctaccggcagatccgctagggataacagggtaatataGATCTGTTTAGCTTGCCTCGTCCCCGCCGGGTCACCCGGCCAGCGACATGGAGGCCCAGAATACCCTCCTTGACAGTCTTGACGTGCGCAGCTCAGGGGCATGATGTGACTGTCGCCCGTACATTTAGCCCATACATCCCCATGTATAATCATTTGCATCCATACATTTTGATGGCCGCACGGCGCGAAGCAAAAATTACGGCTCCTCGCTGCAGACCTGCGAGCAGGGAAACGCTCCCCTCACAGACGCGTTGAATTGTCCCCACGCCGCGCCCCTGTAGAGAAATATAAAAGGTTAGGATTTGCCACTGAGGTTCTTCTTTCATATACTTCCTTTTAAAATCTTGCTAGGATACAGTTCTCACATCACATCCGAACATAAACAACCATGGGTACCACTCTTGACGACACGGCTTACCGGTACCGCACCAGTGTCCCGGGGGACGCCGAGGCCATCGAGGCACTGGATGGGTCCTTCACCACCGACACCGTCTTCCGCGTCACCGCCACCGGGGACGGCTTCACCCTGCGGGAGGTGCCGGTGGACCCGCCCCTGACCAAGGTGTTCCCCGACGACGAATCGGACGACGAATCGGACGACGGGGAGGACGGCGACCCGGACTCCCGGACGTTCGTCGCGTACGGGGACGACGGCGACCTGGCGGGCTTCGTGGTCGTCTCGTACTCCGGCTGGAACCGCCGGCTGACCGTCGAGGACATCGAGGTCGCCCCGGAGCACCGGGGGCACGGGGTCGGGCGCGCGTTGATGGGGCTCGCGACGGAGTTCGCCCGCGAGCGGGGCGCCGGGCACCTCTGGCTGGAGGTCACCAACGTCAACGCACCGGCGATCCACGCGTACCGGCGGATGGGGTTCACCCTCTGCGGCCTGGACACCGCCCTGTACGACGGCACCGCCTCGGACGGCGAGCAGGCGCTCTACATGAGCATGCCCTGCCCCTAATCAGTACTGACAATAAAAAGATTCTTGTTTTCAAGAACTTGTCATTTGTATAGTTTTTTTATATTGTAGTTGTTCTATTTTAATCAAATGTTAGCGTGATTTATATTTTTTTTCGCCTCGACATCATCTGCCCAGATGCGAAGTTAAGTGCGCAGAAAGTAATATCATGCGTCAATCGTATGTGAATGCTGGTCGCTATACTGCTGTCGATTCGATACTAACGCCGCCATCCAGTGTCGAAAACGAGCTCGaattcctgggtccttttcatcacgtgctataaaaataattataatttaaattttttaatataaatatataaattaaaaatagaaagtaaaaaaagaaattaaagaaaaaatagtttttgttttccgaagatgtaaaagactctagggggatcgccaacaaatactaccttttatcttgctcttcctgctctcaggtattaatgccgaattgtttcatcttgtctgtgtagaagaccacacacgaaaatcctgtgattttacattttacttatcgttaatcgaatgtatatctatttaatctgcttttcttgtctaataaatatatatgtaaagtacgctttttgttgaaattttttaaacctttgtttatttttttttcttcattccgtaactcttctaccttctttatttactttctaaaatccaaatacaaaacataaaaataaataaacacagagtaaattcccaaattattccatcattaaaagatacgaggcgcgtgtaagttacaggcaagcgatccgtccGATATCatcagatccactagtggcctatgcggccgcggatctgccggtctccctatagtgagtcgtattaatttcgataagccaggttaacctgcattaatgaatcggccaacgcgcggggagaggcggtttgcgtattgggcgctcttccgcttcctcgctcactgactcgctgcgctcggtcgttcggctgcggcgagcggtatcagctcactcaaaggcggtaatacggttatccacagaatcaggggataacgcaggaaagaacatgtgagcaaaaggccagcaaaaggccaggaaccgtaaaaaggccgcgttgctggcgtttttccataggctccgcccccctgacgagcatcacaaaaatcgacgctcaagtcagaggtggcgaaacccgacaggactataaagataccaggcgtttccccctggaagctccctcgtgcgctctcctgttccgaccctgccgcttaccggatacctgtccgcctttctcccttcgggaagcgtggcgctttctcaTAgctcacgctgtaggtatctcagttcggtgtaggtcgttcgctccaagctgggctgtgtgcacgaaccccccgttcagcccgaccgctgcgccttatccggtaactatcgtcttgagtccaacccggtaagacacgacttatcgccactggcagcagccactggtaacaggattagcagagcgaggtatgtaggcggtgctacagagttcttgaagtggtggcctaactacggctacactagaagAacagtatttggtatctgcgctctgctgaagccagttaccttcggaaaaagagttggtagctcttgatccggcaaacaaaccaccgctggtagcggtggtttttttgtttgcaagcagcagattacgcgcagaaaaaaaggatctcaagaagatcctttgatcttttctacggggtctgacgctcagtggaacgaaaactcacgttaagggattttggtcatgagattatcaaaaaggatcttcacctagatccttttaaattaaaaatgaagttttaaatcaatctaaagtatatatgagtaaacttggtctgacagttaccaatgcttaatcagtgaggcacctatctcagcgatctgtctatttcgttcatccatagttgcctgactccccgtcgtgtagataactacgatacgggagggcttaccatctggccccagtgctgcaatgataccgcgagacccacgTtcaccggctccagatttatcagcaataaaccagccagccggaagggccgagcgcagaagtggtcctgcaactttatccgcctccatccagtctattaattgttgccgggaagctagagtaagtagttcgccagttaatagtttgcgcaacgttgttgccattgctacaggcatcgtggtgtcacgctcgtcgtttggtatggcttcattcagctccggttcccaacgatcaaggcgagttacatgatcccccatgttgtgcaaaaaagcggttagctccttcggtcctccgatcgttgtcagaagtaagttggccgcagtgttatcactcatggttatggcagcactgcataattctcttactgtcatgccatccgtaagatgcttttctgtgactggtgagtactcaaccaagtcattctgagaatagtgtatgcggcgaccgagttgctcttgcccggcgtcaatacgggataataccgcgccacatagcagaactttaaaagtgctcatcattggaaaacgttcttcggggcgaa\"\n",
    "PLASMID = \"N\"*500\n",
    "PLASMID = PLASMID.upper()\n",
    "INSERT_START = PLASMID.find('N'*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c766c5d",
   "metadata": {},
   "source": [
    "# ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f38bb46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T00:22:03.137808Z",
     "start_time": "2023-10-25T00:22:03.124202Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python3 /wynton/home/ahituv/fongsl/EMF/US/ml_emf/bin/train_transfer.py \\n--train_valid_path \"$1\" \\n--foldify \\n--delimiter tab \\n--seed 42 \\n--train_batch_size 1024 \\n--train_workers 8 \\n--valid_batch_size 124 \\n--valid_workers 8 \\n--epoch_num 5 \\n--batch_per_epoch 1000 \\n--weights uniform\\n--seqsize \"$2\" \\n--temp .TEMPDIR  \\n--singleton_definition integer \\n--gpu 0 \\n--model_dir /wynton/home/ahituv/fongsl/EMF/US/ml_emf/bin/model/ \\n--ks 7 --blocks 256 128 128 64 64 64 64 \\n--resize_factor 4 \\n--se_reduction 4 \\n--shift 0.5 \\n--scale 0.5 \\n--loss kl \\n--final_ch 18 \\n--optimizer adamw \\n--model /wynton/home/ahituv/fongsl/EMF/US/ml_emf/data/legnet/model_300.pth\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# commands\n",
    "# qsub -q gpu.q /wynton/home/ahituv/fongsl/EMF/US/ml_emf/bin/train_transfer.sh\n",
    "TRAIN = \"/wynton/home/ahituv/fongsl/EMF/US/data/AGARWAL_RAND5K_WTC11.txt\"\n",
    "SEQ_SIZE = 200\n",
    "MODEL_DIR = \"/wynton/home/ahituv/fongsl/EMF/US/ml_emf/bin/model/\"\n",
    "MODEL=\"/wynton/home/ahituv/fongsl/EMF/US/ml_emf/data/legnet/model_300.pth\"\n",
    "\n",
    "\"\"\"python3 /wynton/home/ahituv/fongsl/EMF/US/ml_emf/bin/train_transfer.py \n",
    "--train_valid_path \"$1\" \n",
    "--foldify \n",
    "--delimiter tab \n",
    "--seed 42 \n",
    "--train_batch_size 1024 \n",
    "--train_workers 8 \n",
    "--valid_batch_size 124 \n",
    "--valid_workers 8 \n",
    "--epoch_num 5 \n",
    "--batch_per_epoch 1000 \n",
    "--weights uniform\n",
    "--seqsize \"$2\" \n",
    "--temp .TEMPDIR  \n",
    "--singleton_definition integer \n",
    "--gpu 0 \n",
    "--model_dir /wynton/home/ahituv/fongsl/EMF/US/ml_emf/bin/model/ \n",
    "--ks 7 --blocks 256 128 128 64 64 64 64 \n",
    "--resize_factor 4 \n",
    "--se_reduction 4 \n",
    "--shift 0.5 \n",
    "--scale 0.5 \n",
    "--loss kl \n",
    "--final_ch 18 \n",
    "--optimizer adamw \n",
    "--model /wynton/home/ahituv/fongsl/EMF/US/ml_emf/data/legnet/model_300.pth\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b781b01e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:03:35.418900Z",
     "start_time": "2023-10-18T20:03:35.394767Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--model'], dest='model', nargs=None, const=None, default='/wynton/home/ahituv/fongsl/EMF/US/ml_emf/data/legnet/model_300.pth', type=<class 'str'>, choices=None, help='path to a .pth file where parameters are stored', metavar=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "\n",
    "parser = ArgumentParser()\n",
    "Path = str\n",
    "parser.add_argument(\"--train_valid_path\",\n",
    "                    default=\"train_val.tsv\", help=\"path to a training dataset\")\n",
    "parser.add_argument(\"--seed\", type=int, default=42,\n",
    "                    help='seed for pseudo-random number generators')\n",
    "parser.add_argument(\"--train_batch_size\", type=int,\n",
    "                    default=1024, help=\"batch size used at training stage\")\n",
    "parser.add_argument(\"--train_workers\", type=int, default=8,\n",
    "                    help=\"number of workers used to read/postprocess data during training\")\n",
    "parser.add_argument(\"--valid_batch_size\", type=int,\n",
    "                    default=4098, help=\"batch size used at validation stage\")\n",
    "parser.add_argument(\"--valid_workers\", type=int, default=8,\n",
    "                    help=\"number of workers used to read/postprocess data during validation\")\n",
    "parser.add_argument(\"--use_sampler\", action=\"store_true\",\n",
    "                    help=\"use weighted sampler\")\n",
    "parser.add_argument(\"--epoch_num\", type=int, default=80,\n",
    "                    help=\"number of training epochs\")\n",
    "parser.add_argument(\"--batch_per_epoch\", type=int, default=1000,\n",
    "                    help=\"epoch is defines as batch_per_epoch batches\")\n",
    "parser.add_argument(\n",
    "    \"--weights\", choices=[\"uniform\", \"counts\"], default=\"uniform\")\n",
    "parser.add_argument(\"--seqsize\", type=int, default=120,\n",
    "                    help=\"sequences will be padded so their length is equal to seqsize\")\n",
    "parser.add_argument(\"--temp\", default=\".TEMPDIR\", type=Path,\n",
    "                    help=\"directory name for auxilarily files\")\n",
    "parser.add_argument(\"--use_single_channel\", action=\"store_true\",\n",
    "                    help=\"use an extra channel to encode singleton information\")\n",
    "parser.add_argument(\"--singleton_definition\",\n",
    "                    choices=[\"integer\", \"threshold1100\"], default=\"integer\", help=\"singleton mode\")\n",
    "parser.add_argument(\"--gpu\", type=int, default=0,\n",
    "                    help=\"ID/number of a GPU device that is used\")\n",
    "parser.add_argument(\"--model_dir\", type=Path, #required=True, \n",
    "                    default = \"/wynton/home/ahituv/fongsl/EMF/US/ml_emf/bin/model/\", \n",
    "                    help=\"directory name where training results are saved\")\n",
    "parser.add_argument(\"--weight_decay\", default=0.01,\n",
    "                    type=float, help=\"weight decay\")\n",
    "parser.add_argument(\"--ks\", default=5, type=int,\n",
    "                    help=\"kernel size of convolutional layers\")\n",
    "parser.add_argument(\"--blocks\", default=[256, 256, 128, 128, 64, 64, 32, 32],\n",
    "                    nargs=\"+\", type=int, help=\"number of channels for EffNet-like blocks\")\n",
    "parser.add_argument(\"--resize_factor\", default=4, type=int,\n",
    "                    help=\"number of channels in a middle/high-dimensional convolutional layer of an EffNet-like block\")\n",
    "parser.add_argument(\"--se_reduction\", default=4, type=float,\n",
    "                    help=\"reduction number used in SELayer\")\n",
    "parser.add_argument(\"--shift\", default=0.5, type=float)\n",
    "parser.add_argument(\"--scale\", default=1.5, type=str)\n",
    "parser.add_argument(\n",
    "    \"--loss\", choices=[\"mse\", \"kl\"], default=\"mse\", type=str, help=\"loss function\")\n",
    "parser.add_argument(\"--final_ch\", default=18, type=int,\n",
    "                    help=\"number of channels of the final convolutional layer\")\n",
    "parser.add_argument(\"--optimizer\", default=\"adamw\",\n",
    "                    choices=[\"adam\", \"adamw\", \"rmsprop\"], help=\"optimizer name\")\n",
    "parser.add_argument(\"--scheduler\", default=\"onecycle\",\n",
    "                    choices=[\"onecycle\"], help=\"scheduler used during optimization\")\n",
    "parser.add_argument(\"--div_factor\", default=25.0, type=float)\n",
    "parser.add_argument(\"--max_lr\", default=0.005, type=float)\n",
    "parser.add_argument(\"--pct_start\", default=0.3, type=float)\n",
    "parser.add_argument(\"--three_phase\", action=\"store_true\")\n",
    "parser.add_argument(\"--float32_approx\", action=\"store_true\")\n",
    "parser.add_argument(\"--bn_momentum\", default=0.1, type=float)\n",
    "parser.add_argument(\"--delimiter\", default='space', type=str,\n",
    "                    help=\"delimiter that separates columns in a training file\")\n",
    "parser.add_argument(\"--foldify\", action=\"store_true\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"/wynton/home/ahituv/fongsl/EMF/US/ml_emf/data/legnet/model_300.pth\",\n",
    "                    help=\"path to a .pth file where parameters are stored\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a8df236",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T00:22:06.815887Z",
     "start_time": "2023-10-25T00:22:06.806269Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args():\n",
    "    train_valid_path=TRAIN\n",
    "    foldify = False\n",
    "    delimiter= \"tab\"\n",
    "    seed=42\n",
    "    train_batch_size=1024\n",
    "    train_workers=8\n",
    "    valid_batch_size=124\n",
    "    valid_workers=8\n",
    "    epoch_num=5\n",
    "    batch_per_epoch=1000\n",
    "    weights=\"uniform\"\n",
    "    seqsize=SEQ_SIZE\n",
    "    temp =\".TEMPDIR \"\n",
    "    singleton_definition=\"integer\"\n",
    "    gpu=0\n",
    "    model_dir=MODEL_DIR\n",
    "    ks=7\n",
    "    blocks=[256, 128, 128, 64, 64, 64, 64]\n",
    "    resize_factor=4\n",
    "    se_reduction=4\n",
    "    shift=0.5\n",
    "    scale=0.5\n",
    "    loss=\"kl\"\n",
    "    final_ch=18\n",
    "    optimizer=\"adamw\"\n",
    "    model=MODEL\n",
    "    float32_approx=False\n",
    "    use_sampler=None\n",
    "    use_single_channel=False\n",
    "    max_lr=0.005\n",
    "    div_factor=25\n",
    "    scheduler = \"onecycle\"\n",
    "args=Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c856eac6",
   "metadata": {},
   "source": [
    "# functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d0b6430",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T00:22:08.745720Z",
     "start_time": "2023-10-25T00:22:08.585135Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def hash_fun(seq, seed):\n",
    "    return mmh3.hash(seq, seed, signed=False) % 10\n",
    "\n",
    "\n",
    "def preprocess_data(data, length):\n",
    "    data = data.copy()\n",
    "    add_part = PLASMID[INSERT_START-length:INSERT_START]\n",
    "    data.seq = data.seq.apply(lambda x:  add_part + x[len(LEFT_ADAPTER):])\n",
    "    data.seq = data.seq.str.slice(-length, None)\n",
    "    return data\n",
    "\n",
    "\n",
    "def set_global_seed(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Sets random seed into PyTorch, TensorFlow, Numpy and Random.\n",
    "    Args:\n",
    "        seed: random seed\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "\n",
    "\n",
    "def n2id(n):\n",
    "    CODES = {\n",
    "        \"A\": 0,\n",
    "        \"T\": 3,\n",
    "        \"G\": 1,\n",
    "        \"C\": 2,\n",
    "        'N': 4\n",
    "    }\n",
    "\n",
    "    return CODES[n.upper()]\n",
    "\n",
    "\n",
    "def id2n(i):\n",
    "    CODES = {\n",
    "        \"A\": 0,\n",
    "        \"T\": 3,\n",
    "        \"G\": 1,\n",
    "        \"C\": 2,\n",
    "        'N': 4\n",
    "    }\n",
    "\n",
    "    INV_CODES = {value: key for key, value in CODES.items()}\n",
    "    return INV_CODES[i]\n",
    "\n",
    "\n",
    "def n2compl(n):\n",
    "\n",
    "    COMPL = {\n",
    "        'A': 'T',\n",
    "        'T': 'A',\n",
    "        'G': 'C',\n",
    "        'C': 'G',\n",
    "        'N': 'N'\n",
    "    }\n",
    "    return COMPL[n.upper()]\n",
    "\n",
    "\n",
    "class Seq2Tensor(nn.Module):\n",
    "    '''\n",
    "    Encode sequences using one-hot encoding after preprocessing.\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, seq: str) -> torch.Tensor:\n",
    "        seq_i = [n2id(x) for x in seq]\n",
    "        code = torch.from_numpy(np.array(seq_i))\n",
    "        code = F.one_hot(code, num_classes=5)  # 5th class is N\n",
    "\n",
    "        code = code[:, :5].float()\n",
    "        code[code[:, 4] == 1] = 0.25  # encode Ns with .25\n",
    "        code = code[:, :4]\n",
    "        return code.transpose(0, 1)\n",
    "\n",
    "\n",
    "def parameter_count(model):\n",
    "    pars = 0\n",
    "    for _, p in model.named_parameters():\n",
    "        pars += torch.prod(torch.tensor(p.shape))\n",
    "    return pars\n",
    "\n",
    "\n",
    "class SeqDatasetRev(Dataset):\n",
    "    def __init__(self, ds_rev, size, use_single_channel):\n",
    "        self.ds = ds_rev\n",
    "        self.size = size\n",
    "        self.use_single_channel = use_single_channel\n",
    "        self.totensor = Seq2Tensor()\n",
    "\n",
    "    def transform(self, x):\n",
    "        assert isinstance(x, str)\n",
    "        assert len(x) == self.size\n",
    "        return self.totensor(x)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        seq = self.transform(self.ds.seq.values[i])\n",
    "        rev = torch.full(\n",
    "            (1, self.size), self.ds.rev.values[i], dtype=torch.float32)\n",
    "\n",
    "        if self.use_single_channel:\n",
    "            single = torch.full(\n",
    "                (1, self.size), self.ds.is_singleton.values[i], dtype=torch.float32)\n",
    "            X = torch.concat([seq, rev, single])\n",
    "        else:\n",
    "            X = torch.concat([seq, rev], dim=0)\n",
    "\n",
    "        bin = self.ds.bin.values[i]\n",
    "        return X, bin\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds.seq)\n",
    "\n",
    "\n",
    "POINTS = np.array([-np.inf, *range(1, 18, 1), np.inf])\n",
    "\n",
    "\n",
    "class SeqDatasetRevProb(Dataset):\n",
    "    def __init__(self, ds_rev, size, use_single_channel, shift=0.5, scale=1.5):\n",
    "        self.ds = ds_rev\n",
    "        self.size = size\n",
    "        self.totensor = Seq2Tensor()\n",
    "        try:\n",
    "            self.scale = float(scale)\n",
    "        except ValueError:\n",
    "            self.scale = scale\n",
    "            print(\"Using adaptive scale\")\n",
    "        self.shift = shift\n",
    "        self.use_single_channel = use_single_channel\n",
    "\n",
    "    def transform(self, x):\n",
    "        assert isinstance(x, str)\n",
    "        assert len(x) == self.size\n",
    "        return self.totensor(x)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        seq = self.transform(self.ds.seq.values[i])\n",
    "        rev = torch.full(\n",
    "            (1, self.size), self.ds.rev.values[i], dtype=torch.float32)\n",
    "        if self.use_single_channel:\n",
    "            single = torch.full(\n",
    "                (1, self.size), self.ds.is_singleton.values[i], dtype=torch.float32)\n",
    "            X = torch.concat([seq, rev, single], dim=0)\n",
    "        else:\n",
    "            X = torch.concat([seq, rev], dim=0)\n",
    "        bin = self.ds.bin.values[i]\n",
    "        if isinstance(self.scale, float):\n",
    "            norm = scipy.stats.norm(loc=bin + self.shift, scale=self.scale)\n",
    "        elif self.scale == \"adaptive\":\n",
    "            s = -0.1364 * bin + 2.7727\n",
    "            norm = scipy.stats.norm(loc=bin + self.shift, scale=s)\n",
    "        else:\n",
    "            raise Exception(\"Wrong scale\")\n",
    "\n",
    "        cumprobs = norm.cdf(POINTS)\n",
    "        probs = cumprobs[1:] - cumprobs[:-1]\n",
    "        return X, probs, bin\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ds.seq)\n",
    "\n",
    "\n",
    "class CustomWeightedRandomSampler(Sampler[int]):\n",
    "    r\"\"\"Samples elements from ``[0,..,len(weights)-1]`` with given probabilities (weights).\n",
    "    Args:\n",
    "        weights (sequence)   : a sequence of weights, not necessary summing up to one\n",
    "        num_samples (int): number of samples to draw\n",
    "        replacement (bool): if ``True``, samples are drawn with replacement.\n",
    "            If not, they are drawn without replacement, which means that when a\n",
    "            sample index is drawn for a row, it cannot be drawn again for that row.\n",
    "        generator (Generator): Generator used in sampling.\n",
    "    Example:\n",
    "        >>> list(WeightedRandomSampler([0.1, 0.9, 0.4, 0.7, 3.0, 0.6], 5, replacement=True))\n",
    "        [4, 4, 1, 4, 5]\n",
    "        >>> list(WeightedRandomSampler([0.9, 0.4, 0.05, 0.2, 0.3, 0.1], 5, replacement=False))\n",
    "        [0, 1, 4, 3, 2]\n",
    "    \"\"\"\n",
    "    weights: Tensor\n",
    "    num_samples: int\n",
    "    replacement: bool\n",
    "\n",
    "    SAMPLES_PER_GROUP: ClassVar[int] = 2 ** 12\n",
    "\n",
    "    def __init__(self, weights: Sequence[float], num_samples: int,\n",
    "                 replacement: bool = True, generator=None) -> None:\n",
    "        if not isinstance(num_samples, int) or isinstance(num_samples, bool) or \\\n",
    "                num_samples <= 0:\n",
    "            raise ValueError(\"num_samples should be a positive integer \"\n",
    "                             \"value, but got num_samples={}\".format(num_samples))\n",
    "        if not isinstance(replacement, bool):\n",
    "            raise ValueError(\"replacement should be a boolean value, but got \"\n",
    "                             \"replacement={}\".format(replacement))\n",
    "\n",
    "        self.weights = torch.as_tensor(weights, dtype=torch.double)\n",
    "        self.num_samples = num_samples\n",
    "        self.replacement = replacement\n",
    "        self.generator = generator\n",
    "        self.groups, self.group_weights, self.inner_weights = self.groups_split()\n",
    "\n",
    "    def __iter__(self) -> Iterator[int]:\n",
    "        group_ids = torch.multinomial(\n",
    "            self.group_weights, self.num_samples, replacement=True, generator=self.generator)\n",
    "        gr, counts = torch.unique(group_ids, sorted=True, return_counts=True)\n",
    "        smpls = []\n",
    "        for i in range(len(gr)):\n",
    "            g = gr[i]\n",
    "            rand_tensor = torch.multinomial(\n",
    "                self.inner_weights[g], counts[i], self.replacement, generator=self.generator)\n",
    "            idxs = self.groups[g][0] + rand_tensor\n",
    "            smpls.append(idxs)\n",
    "        rand_tensor = torch.concat(smpls)\n",
    "        pi = torch.randperm(rand_tensor.shape[0])\n",
    "        rand_tensor = rand_tensor[pi]\n",
    "        yield from iter(rand_tensor.tolist())\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.num_samples\n",
    "\n",
    "    def groups_split(self):\n",
    "        groups = []\n",
    "        N = len(self.weights)\n",
    "        for start in range(0, N, self.SAMPLES_PER_GROUP):\n",
    "            end = min(start+self.SAMPLES_PER_GROUP, N)\n",
    "            groups.append(torch.arange(start, end, 1, dtype=torch.long))\n",
    "        inner_weights = [self.weights[ids] for ids in groups]\n",
    "        weights = torch.FloatTensor([w.sum() for w in inner_weights])\n",
    "        return groups, weights, inner_weights\n",
    "\n",
    "\n",
    "def get_weights(df, tp):\n",
    "    if tp == \"uniform\":\n",
    "        weights = np.full_like(df.bin.values, fill_value=1)\n",
    "        return weights\n",
    "    elif tp == \"counts\":\n",
    "        weights = df.cnt.values\n",
    "        return weights\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "class DataloaderWrapper:\n",
    "    def __init__(self, dataloader, batch_per_epoch):\n",
    "        self.batch_per_epoch = batch_per_epoch\n",
    "        self.dataloader = dataloader\n",
    "        self.iterator = iter(dataloader)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.batch_per_epoch\n",
    "\n",
    "    def __next__(self):\n",
    "        try:\n",
    "            return next(self.iterator)\n",
    "        except StopIteration:\n",
    "            self.iterator = iter(self.dataloader)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for _ in range(self.batch_per_epoch):\n",
    "            try:\n",
    "                yield next(self.iterator)\n",
    "            except StopIteration:\n",
    "                self.iterator = iter(self.dataloader)\n",
    "\n",
    "\n",
    "def revcomp(seq):\n",
    "    return \"\".join((n2compl(x) for x in reversed(seq)))\n",
    "\n",
    "\n",
    "class PearsonMetric(Metric):\n",
    "    def __init__(self, output_transform=lambda x: x, device=\"cpu\"):\n",
    "        self._ys = []\n",
    "        self._ypreds = []\n",
    "        super().__init__(output_transform=output_transform, device=device)\n",
    "\n",
    "    def reset(self):\n",
    "        self._ys = []\n",
    "        self._ypreds = []\n",
    "        super().reset()\n",
    "\n",
    "    def update(self, output):\n",
    "        y_pred, y = output[0].cpu().numpy(), output[1].cpu().numpy()\n",
    "        self._ys.append(y)\n",
    "        self._ypreds.append(y_pred)\n",
    "\n",
    "    def compute(self):\n",
    "        y = np.concatenate(self._ys)\n",
    "        y_pred = np.concatenate(self._ypreds)\n",
    "        cor, _ = pearsonr(y, y_pred)\n",
    "        return cor\n",
    "\n",
    "\n",
    "class SpearmanMetric(Metric):\n",
    "    def __init__(self, output_transform=lambda x: x, device=\"cpu\"):\n",
    "        self._ys = []\n",
    "        self._ypreds = []\n",
    "        super().__init__(output_transform=output_transform, device=device)\n",
    "\n",
    "    def reset(self):\n",
    "        self._ys = []\n",
    "        self._ypreds = []\n",
    "        super().reset()\n",
    "\n",
    "    def update(self, output):\n",
    "        y_pred, y = output[0].cpu().numpy(), output[1].cpu().numpy()\n",
    "        self._ys.append(y)\n",
    "        self._ypreds.append(y_pred)\n",
    "\n",
    "    def compute(self):\n",
    "        y = np.concatenate(self._ys)\n",
    "        y_pred = np.concatenate(self._ypreds)\n",
    "        cor, _ = spearmanr(y, y_pred)\n",
    "        return cor\n",
    "\n",
    "\n",
    "def create_trainer(model,\n",
    "                   optimizer,\n",
    "                   scheduler,\n",
    "                   criterion,\n",
    "                   device,\n",
    "                   model_dir):\n",
    "    model_dir = Path(model_dir)\n",
    "    model_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    train_mse = MeanSquaredError()\n",
    "    train_pearson = PearsonMetric()\n",
    "    train_spearman = SpearmanMetric()\n",
    "\n",
    "    def train_step(trainer, batch):\n",
    "        nonlocal model\n",
    "        if not model.training:\n",
    "            model = model.train()\n",
    "        X, y_probs, y = batch\n",
    "        X = X.to(device)\n",
    "        y_probs = y_probs.float().to(device)\n",
    "        logprobs, y_pred = model(X)\n",
    "        loss = criterion(logprobs, y_probs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        scheduler.step()\n",
    "        out = (y_pred.detach().cpu(), y)\n",
    "        train_mse.update(out)\n",
    "        train_pearson.update(out)\n",
    "        train_spearman.update(out)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    trainer = Engine(train_step)\n",
    "\n",
    "    @trainer.on(Events.STARTED)\n",
    "    def prepare_epoch(engine):\n",
    "        engine.state.metrics['train_pearson'] = -np.inf\n",
    "        engine.state.metrics['train_mse'] = -np.inf\n",
    "        engine.state.metrics['train_spearman'] = -np.inf\n",
    "\n",
    "    def evaluate(engine, batch):\n",
    "        nonlocal model\n",
    "        if model.training:\n",
    "            model = model.eval()\n",
    "        with torch.no_grad():\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.float().to(device)\n",
    "            _, y_pred = model(X, predict_score=True)\n",
    "        return y_pred.cpu(), y.cpu()\n",
    "\n",
    "    evaluator = Engine(evaluate)\n",
    "\n",
    "    MeanSquaredError().attach(evaluator, 'mse')\n",
    "    p = PearsonMetric()\n",
    "    p.attach(evaluator, 'pearson')\n",
    "    s = SpearmanMetric()\n",
    "    s.attach(evaluator, 'spearman')\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)  # | Events.STARTED)\n",
    "    def validate(engine):\n",
    "        p.reset()\n",
    "\n",
    "        engine.state.metrics['train_mse'] = train_mse.compute()\n",
    "        engine.state.metrics['train_pearson'] = train_pearson.compute()\n",
    "        engine.state.metrics['train_spearman'] = train_spearman.compute()\n",
    "        train_mse.reset()\n",
    "        train_pearson.reset()\n",
    "        train_spearman.reset()\n",
    "\n",
    "        score_path = model_dir / f\"scores_{engine.state.epoch}.json\"\n",
    "        with open(score_path, \"w\") as outp:\n",
    "            json.dump(engine.state.metrics, outp)\n",
    "\n",
    "    @trainer.on(Events.EPOCH_COMPLETED)\n",
    "    def dump_model(engine):\n",
    "        model_path = model_dir / f\"model_{engine.state.epoch}.pth\"\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        optimizer_path = model_dir / f\"optimizer_{engine.state.epoch}.pth\"\n",
    "        torch.save(optimizer.state_dict(), optimizer_path)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler_path = model_dir / f\"scheduler_{engine.state.epoch}.pth\"\n",
    "            torch.save(scheduler.state_dict(), scheduler_path)\n",
    "\n",
    "    pbar = ProgressBar(persist=True)\n",
    "    pbar.attach(trainer, [\"train_mse\", \"train_pearson\", \"train_spearman\", ],\n",
    "                output_transform=lambda x: {'batch_loss': x},\n",
    "                )\n",
    "    return trainer, p\n",
    "\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Conv1d):\n",
    "        n = m.kernel_size[0] * m.out_channels\n",
    "        m.weight.data.normal_(0, math.sqrt(2 / n))\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight.data, 1)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        m.weight.data.normal_(0, 0.001)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "def add_rev(df):\n",
    "    df = df.copy()\n",
    "    revdf = df.copy()\n",
    "    revdf['seq'] = df.seq.apply(revcomp)\n",
    "    df['rev'] = 0\n",
    "    revdf['rev'] = 1\n",
    "    df = pd.concat([df, revdf]).reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def infer_singleton(arr, method):\n",
    "    print(arr)\n",
    "    if method == \"integer\":\n",
    "        return np.array([x.is_integer() for x in arr])\n",
    "    elif method.startswith(\"threshold\"):\n",
    "        th = float(method.replace(\"threshold\", \"\"))\n",
    "        cnt = Counter(arr)\n",
    "        return np.array([cnt[x] >= th for x in arr])\n",
    "    else:\n",
    "        raise Exception(\"Wrong method\")\n",
    "\n",
    "\n",
    "def add_singleton_column(df, method):\n",
    "    df = df.copy()\n",
    "    df[\"is_singleton\"] = infer_singleton(df.bin.values, method)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2110584",
   "metadata": {},
   "source": [
    "# run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4776b19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T00:22:17.759290Z",
     "start_time": "2023-10-25T00:22:17.720374Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'mkdir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c1d6133e4bf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0margs_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"args.json\"\u001b[0m  \u001b[0;31m# changed from /\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'mkdir'"
     ]
    }
   ],
   "source": [
    "torch.backends.cuda.matmul.allow_tf32 = args.float32_approx  # default is False\n",
    "\n",
    "if args.use_sampler and args.weights is None:\n",
    "    args.weights = None\n",
    "\n",
    "args.model_dir.mkdir(exist_ok=False, parents=True)\n",
    "\n",
    "args_path = args.model_dir + \"args.json\"  # changed from /\n",
    "args_path\n",
    "\n",
    "with open(args_path, 'w') as outp:  # changed args_path.open\n",
    "    dt = {x: str(y) if isinstance(y, Path)\n",
    "          else y for x, y in vars(args).items()}\n",
    "    json.dump(dt, outp, indent=4)\n",
    "\n",
    "run_backup_path = args.model_dir + \"run.py\" # changed from /\n",
    "\n",
    "shutil.copy(sys.argv[0], run_backup_path)  # skipped\n",
    "shutil.copy(os.path.join(os.path.dirname(\n",
    "    sys.argv[0]), 'model.py'), os.path.join(args.model_dir, 'model.py'))\n",
    "\n",
    "if os.path.exists(args.temp):  # changed\n",
    "    args.temp.mkdir()\n",
    "\n",
    "# make training and validation files\n",
    "FROM = os.path.split(args.train_valid_path)[1]\n",
    "train_path = args.temp + \\\n",
    "    f\"train_{args.seqsize}_{args.singleton_definition}_from_{FROM}.txt\"\n",
    "valid_path = args.temp + \\\n",
    "    f\"valid_{args.seqsize}_{args.singleton_definition}_from_{FROM}.txt\"\n",
    "\n",
    "# load input data\n",
    "if not (train_path.exists() and valid_path.exists()):\n",
    "\n",
    "    # open validation data\n",
    "    train_valid = pd.read_table(args.train_valid_path,\n",
    "                                sep='\\t' if args.delimiter == 'tab' else ' ',\n",
    "                                header=None)  # modified\n",
    "\n",
    "    # throw error if no activity value\n",
    "    err_str = f\"No bin column in a training dataset! \\\n",
    "    Make sure that the --delimiter argument is correct \\\n",
    "    (tab or space, current: {args.delimiter}).\"\n",
    "\n",
    "    assert len(train_valid.columns) >= 2, err_str\n",
    "\n",
    "    # rename columns\n",
    "    train_valid.columns = ['seq', 'bin', 'fold'][:len(train_valid.columns)]\n",
    "\n",
    "    # break into folds?\n",
    "    if args.foldify and ('fold' not in train_valid):\n",
    "        fold = list(map(lambda x: hash_fun(x, args.seed), train_valid.seq))\n",
    "        train_valid['fold'] = fold\n",
    "        train_valid = train_valid.sort_values('fold')\n",
    "\n",
    "    print(train_valid.head())\n",
    "\n",
    "    # preprocess data\n",
    "    train_valid = preprocess_data(train_valid, args.seqsize)\n",
    "\n",
    "    # set up training and reverse training data.\n",
    "    if args.use_single_channel:\n",
    "        train_valid = add_singleton_column(\n",
    "            train_valid, args.singleton_definition)\n",
    "\n",
    "    train = train_valid\n",
    "\n",
    "    train = add_rev(train)\n",
    "    train.to_csv(train_path, sep=\"\\t\", index=False, header=True)  # write data\n",
    "\n",
    "else:\n",
    "    train = pd.read_table(train_path)\n",
    "\n",
    "# set up training tensor\n",
    "train_ds = SeqDatasetRevProb(train,\n",
    "                             size=args.seqsize,\n",
    "                             use_single_channel=args.use_single_channel,\n",
    "                             shift=args.shift,\n",
    "                             scale=args.scale)\n",
    "\n",
    "\n",
    "set_global_seed(args.seed)\n",
    "\n",
    "#\n",
    "if args.use_sampler:\n",
    "    print(\"Using custom sampler\")\n",
    "    weights = get_weights(train, args.weights)\n",
    "    train_per_epoch_size = args.train_batch_size * args.batch_per_epoch\n",
    "    sampler = CustomWeightedRandomSampler(weights=weights,\n",
    "                                          num_samples=train_per_epoch_size)\n",
    "    train_dl = DataLoader(train_ds,\n",
    "                          batch_size=args.train_batch_size,\n",
    "                          num_workers=args.train_workers,\n",
    "                          sampler=sampler,\n",
    "                          shuffle=False)\n",
    "else:\n",
    "    print(\"Using shuffle\")\n",
    "    train_dl = DataLoader(train_ds,\n",
    "                          batch_size=args.train_batch_size,\n",
    "                          num_workers=args.train_workers,\n",
    "                          shuffle=True)\n",
    "    train_dl = DataloaderWrapper(train_dl, args.batch_per_epoch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b28dab",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-25T00:23:19.613Z"
    }
   },
   "outputs": [],
   "source": [
    "###\n",
    "# Load model\n",
    "###\n",
    "\n",
    "print(\"Loading model...\")\n",
    "device = torch.device(f\"cuda:{args.gpu}\")  # get device (CUDA or CPU)\n",
    "print(\"cuda?\", torch.cuda.is_available())\n",
    "print(device)\n",
    "\n",
    "###\n",
    "# instantiate model\n",
    "###\n",
    "model = SeqNN(seqsize=args.seqsize, use_single_channel=args.use_single_channel, block_sizes=args.blocks, ks=args.ks,\n",
    "              resize_factor=args.resize_factor, se_reduction=args.se_reduction, final_ch=args.final_ch).to(device)\n",
    "\n",
    "# load model\n",
    "model.load_state_dict(torch.load(args.model, map_location=\"cpu\"))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d385674",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T20:13:45.530840Z",
     "start_time": "2023-10-18T20:13:45.528012Z"
    }
   },
   "outputs": [],
   "source": [
    "# figure out number of filters in fully connected layer\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# add new fully connected layer with number of nodes using args.final_ch\n",
    "model.fc = nn.Linear(num_ftrs, args.final_ch)\n",
    "\n",
    "# push new model to device\n",
    "model = model.to(device)\n",
    "# model.apply(initialize_weights)  # for training from scratch, but not for transfer learning.\n",
    "\n",
    "min_lr = args.max_lr / args.div_factor  # set the minimum learning rate.\n",
    "\n",
    "###\n",
    "# OPTIMIZER\n",
    "###\n",
    "\n",
    "if args.optimizer == \"adam\":\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=min_lr, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == \"adamw\":\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(), lr=min_lr, weight_decay=args.weight_decay)\n",
    "elif args.optimizer == \"rmsprop\":\n",
    "    optimizer = torch.optim.RMSprop(\n",
    "        model.parameters(), lr=min_lr, weight_decay=args.weight_decay)\n",
    "else:\n",
    "    raise Exception(\"Wrong optimizer\")\n",
    "\n",
    "###\n",
    "# CRITERION\n",
    "###\n",
    "\n",
    "if args.loss == \"kl\":\n",
    "    criterion = nn.KLDivLoss(reduction=\"batchmean\").to(device)\n",
    "elif args.loss == \"mse\":\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "else:\n",
    "    raise Exception(\"Wrong loss\")\n",
    "\n",
    "# set model directory\n",
    "model_dir = args.model_dir\n",
    "log_dir = model_dir + \"logs\"\n",
    "\n",
    "if args.scheduler == \"onecycle\":\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                    max_lr=args.max_lr,\n",
    "                                                    steps_per_epoch=args.batch_per_epoch,\n",
    "                                                    epochs=args.epoch_num,\n",
    "                                                    pct_start=args.pct_start,\n",
    "                                                    three_phase=args.three_phase,\n",
    "                                                    div_factor=args.div_factor)\n",
    "else:\n",
    "    raise Exception(\"Wrong scheduler type\")\n",
    "\n",
    "print('Parameters:', int(parameter_count(model)))\n",
    "trainer, p = create_trainer(model, optimizer, scheduler, criterion, device,\n",
    "                            model_dir)\n",
    "\n",
    "tb_logger = TensorboardLogger(log_dir=log_dir)\n",
    "tb_logger.attach_output_handler(\n",
    "    trainer,\n",
    "    event_name=Events.ITERATION_COMPLETED,\n",
    "    tag=\"training\",\n",
    "    output_transform=lambda loss: {\"batchloss\": loss},\n",
    ")\n",
    "\n",
    "state = trainer.run(train_dl, max_epochs=args.epoch_num)  # type: ignore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
